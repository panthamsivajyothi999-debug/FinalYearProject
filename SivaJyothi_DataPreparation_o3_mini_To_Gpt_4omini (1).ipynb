{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuLVG_2YZpB0"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r63nM0ukaR5W"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "O3-MINI SPEED-OPTIMIZED Reasoning Pipeline for Google Colab\n",
        "Enhanced performance while maintaining o3-mini model\n",
        "\n",
        "KEY OPTIMIZATIONS FOR O3-MINI:\n",
        "- Medium/Low reasoning effort configuration\n",
        "- Reduced output tokens for faster processing\n",
        "- Removed artificial delays\n",
        "- Concurrent processing with multiple parallel requests\n",
        "- Optimized prompts for efficiency\n",
        "- Intelligent batching and error handling\n",
        "- Complete response logging system\n",
        "- Enhanced email quality selection\n",
        "\n",
        "IMPROVEMENTS:\n",
        "- Single email processing time significantly reduced\n",
        "- Concurrent processing capabilities\n",
        "- Cost reduction through optimization\n",
        "- Comprehensive logging and monitoring\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "from datetime import datetime, timedelta\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import logging\n",
        "import concurrent.futures\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import pickle\n",
        "import threading\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# Google Colab specific imports\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "class O3MiniOptimizedPipeline:\n",
        "    \"\"\"O3-Mini optimized pipeline with enhanced performance and quality selection\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataset_path=\"Enron.csv\",\n",
        "                 samples_per_class=5000,\n",
        "                 output_dir=\"output\",\n",
        "                 gdrive_base_path=\"/content/drive/MyDrive/phishing_detection_final\",#\n",
        "                 checkpoint_interval=50,\n",
        "                 max_concurrent=8,\n",
        "                 api_key=None,\n",
        "                 o3_speed_mode=\"medium\"):   # low,medium,high\n",
        "        \"\"\"Initialize the O3-Mini speed-optimized pipeline\"\"\"\n",
        "\n",
        "        # O3-Mini speed configuration\n",
        "        self.o3_speed_mode = o3_speed_mode\n",
        "        self.max_concurrent = max_concurrent\n",
        "\n",
        "        # O3-Mini specific settings\n",
        "        if o3_speed_mode == \"low\":\n",
        "            self.reasoning_effort = \"low\"\n",
        "            self.max_output_tokens = 8000\n",
        "            self.checkpoint_interval = 100 #\n",
        "            print(\"O3-Mini LOW effort mode: Maximum processing speed\")\n",
        "        elif o3_speed_mode == \"medium\":\n",
        "            self.reasoning_effort = \"medium\"\n",
        "            self.max_output_tokens = 12000\n",
        "            self.checkpoint_interval = 50 #\n",
        "            print(\"O3-Mini MEDIUM effort mode: Balanced speed and accuracy\")\n",
        "        else:  # high mode\n",
        "            self.reasoning_effort = \"high\"\n",
        "            self.max_output_tokens = 20000\n",
        "            self.checkpoint_interval = 25\n",
        "            print(\"O3-Mini HIGH effort mode: Maximum accuracy\")\n",
        "\n",
        "        # Basic configuration\n",
        "        self.dataset_path = dataset_path\n",
        "        self.samples_per_class = samples_per_class\n",
        "        self.total_samples = samples_per_class * 2\n",
        "\n",
        "        # Google Drive configuration\n",
        "        self.gdrive_base_path = gdrive_base_path\n",
        "        self.use_gdrive = IN_COLAB\n",
        "\n",
        "        if self.use_gdrive:\n",
        "            self._mount_google_drive()\n",
        "            self.output_dir = os.path.join(self.gdrive_base_path, output_dir)\n",
        "        else:\n",
        "            self.output_dir = output_dir\n",
        "\n",
        "        # Create directory structure\n",
        "        self.final_dataset_path = os.path.join(self.output_dir, \"final_datasets\")\n",
        "        self.intermediate_path = os.path.join(self.output_dir, \"intermediate\")\n",
        "        self.logs_path = os.path.join(self.output_dir, \"logs\")\n",
        "        self.checkpoint_path = os.path.join(self.output_dir, \"checkpoints\")\n",
        "        self.response_logs_path = os.path.join(self.output_dir, \"response_logs\")\n",
        "\n",
        "        self._create_output_directories()\n",
        "        self._setup_logging()\n",
        "\n",
        "        # API configuration\n",
        "        if api_key:\n",
        "            self.client = OpenAI(api_key=api_key)#\n",
        "        else:\n",
        "            # Default API key from original code\n",
        "            api_key = \"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\"\n",
        "            self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "        # Concurrent processing setup\n",
        "        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.max_concurrent)\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "        # Pipeline tracking\n",
        "        self.processed_count = 0\n",
        "        self.successful_reasoning = 0\n",
        "        self.failed_reasoning = 0\n",
        "        self.total_tokens_used = 0\n",
        "        self.total_processing_time = 0\n",
        "        self.start_time = None\n",
        "\n",
        "        # Results storage\n",
        "        self.enhanced_dataset = []\n",
        "        self.processing_stats = []\n",
        "\n",
        "        # Retry configuration\n",
        "        self.max_retries = 2\n",
        "        self.retry_delay = 2\n",
        "\n",
        "        logging.info(f\"O3-Mini Optimized Pipeline Initialized\")\n",
        "        logging.info(f\"Reasoning effort: {self.reasoning_effort}\")\n",
        "        logging.info(f\"Max tokens: {self.max_output_tokens}\")\n",
        "        logging.info(f\"Max concurrent: {self.max_concurrent}\")\n",
        "        logging.info(f\"Target: {self.total_samples} samples ({samples_per_class} per class)\")\n",
        "        logging.info(f\"Response logging enabled - Raw responses saved to {self.response_logs_path}\")\n",
        "\n",
        "    def _mount_google_drive(self):\n",
        "        \"\"\"Mount Google Drive in Colab\"\"\"\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            logging.info(\"Google Drive mounted successfully\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to mount Google Drive: {e}\")\n",
        "            self.use_gdrive = False\n",
        "\n",
        "    def _create_output_directories(self):\n",
        "        \"\"\"Create output directories\"\"\"\n",
        "        directories = [\n",
        "            self.output_dir,\n",
        "            self.final_dataset_path,\n",
        "            self.intermediate_path,\n",
        "            self.logs_path,\n",
        "            self.checkpoint_path,\n",
        "            self.response_logs_path\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        log_file = os.path.join(self.logs_path, f'o3_mini_optimized_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
        "\n",
        "        for handler in logging.root.handlers[:]:\n",
        "            logging.root.removeHandler(handler)\n",
        "\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(log_file, encoding='utf-8'),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def create_optimized_prompt(self, subject, body, label):\n",
        "        \"\"\"Speed-optimized prompt for O3-Mini\"\"\"\n",
        "\n",
        "        actual_class = \"LEGITIMATE\" if label == 0 else \"PHISHING\"\n",
        "\n",
        "        prompt = f\"\"\"You are a cybersecurity expert analyzing emails for phishing indicators.\n",
        "\n",
        "EMAIL:\n",
        "Subject: {subject}\n",
        "Body: {body}\n",
        "\n",
        "GROUND TRUTH: This email is {actual_class}\n",
        "\n",
        "Provide focused analysis in exactly 5 sections (2-3 sentences each):\n",
        "\n",
        "1. SENDER ANALYSIS\n",
        "Examine sender authenticity, domain reputation, and authentication indicators.\n",
        "\n",
        "2. LANGUAGE PATTERNS\n",
        "Analyze grammar, writing style, and linguistic deception markers.\n",
        "\n",
        "3. SOCIAL ENGINEERING\n",
        "Identify psychological manipulation tactics and emotional triggers.\n",
        "\n",
        "4. TECHNICAL INDICATORS\n",
        "Assess URLs, requests, attachments, and technical attack vectors.\n",
        "\n",
        "5. RISK ASSESSMENT\n",
        "Correlate findings with threat intelligence and provide final assessment.enro\n",
        "\n",
        "CLASSIFICATION: [Write either LEGITIMATE or PHISHING]   #classification:phishing\n",
        "CONFIDENCE: [Write either HIGH, MEDIUM, or LOW]\n",
        "\n",
        "Keep sections concise but thorough. Focus on critical indicators.\"\"\"\n",
        "\n",
        "        return prompt.strip()\n",
        "\n",
        "    def _save_response_log(self, response, email_index, subject, body, label, request_time, prompt=None):\n",
        "        \"\"\"Save raw API response as JSON log for debugging and analysis\"\"\"\n",
        "        try:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
        "            log_filename = os.path.join(self.response_logs_path, f\"response_email_{email_index:05d}_{timestamp}.json\")\n",
        "\n",
        "            # Create response log data\n",
        "            response_log = {\n",
        "                'email_metadata': {\n",
        "                    'email_index': email_index,\n",
        "                    'subject_preview': subject[:100] + '...' if len(subject) > 100 else subject,\n",
        "                    'body_length': len(body),\n",
        "                    'actual_label': label,\n",
        "                    'actual_label_name': \"LEGITIMATE\" if label == 0 else \"PHISHING\",\n",
        "                    'processing_timestamp': datetime.now().isoformat(),\n",
        "                    'request_time_seconds': request_time\n",
        "                },\n",
        "                'api_request': {\n",
        "                    'model': 'o3-mini',\n",
        "                    'reasoning_effort': self.reasoning_effort,\n",
        "                    'max_output_tokens': self.max_output_tokens,\n",
        "                    'prompt_length': len(prompt) if prompt else 0,\n",
        "                    'prompt_preview': prompt[:500] + '...' if prompt and len(prompt) > 500 else prompt\n",
        "                },\n",
        "                'api_response': {\n",
        "                    'response_id': getattr(response, 'id', None),\n",
        "                    'object_type': getattr(response, 'object', None),\n",
        "                    'model': getattr(response, 'model', None),\n",
        "                    'status': getattr(response, 'status', None),\n",
        "                    'created': getattr(response, 'created', None),\n",
        "                    'output_text_length': len(response.output_text) if hasattr(response, 'output_text') and response.output_text else 0,\n",
        "                    'output_text_preview': response.output_text[:1000] + '...' if hasattr(response, 'output_text') and response.output_text and len(response.output_text) > 1000 else (response.output_text if hasattr(response, 'output_text') else None)\n",
        "                },\n",
        "                'token_usage': {},\n",
        "                'reasoning_data': {},\n",
        "                'incomplete_details': None\n",
        "            }\n",
        "\n",
        "            # Extract token usage details\n",
        "            if hasattr(response, 'usage') and response.usage:\n",
        "                usage = response.usage\n",
        "                response_log['token_usage'] = {\n",
        "                    'input_tokens': getattr(usage, 'input_tokens', 0),\n",
        "                    'output_tokens': getattr(usage, 'output_tokens', 0),\n",
        "                    'total_tokens': getattr(usage, 'total_tokens', 0)\n",
        "                }\n",
        "\n",
        "                # Extract reasoning token details\n",
        "                if hasattr(usage, 'output_tokens_details') and usage.output_tokens_details:\n",
        "                    details = usage.output_tokens_details\n",
        "                    reasoning_tokens = getattr(details, 'reasoning_tokens', 0) or 0\n",
        "                    visible_tokens = response_log['token_usage']['output_tokens'] - reasoning_tokens\n",
        "\n",
        "                    response_log['reasoning_data'] = {\n",
        "                        'reasoning_tokens': reasoning_tokens,\n",
        "                        'visible_tokens': visible_tokens,\n",
        "                        'reasoning_ratio_percent': (reasoning_tokens / response_log['token_usage']['output_tokens'] * 100) if response_log['token_usage']['output_tokens'] > 0 else 0\n",
        "                    }\n",
        "\n",
        "            # Handle incomplete responses\n",
        "            if hasattr(response, 'status') and response.status == 'incomplete':\n",
        "                if hasattr(response, 'incomplete_details'):\n",
        "                    response_log['incomplete_details'] = {\n",
        "                        'reason': getattr(response.incomplete_details, 'reason', None) if response.incomplete_details else None\n",
        "                    }\n",
        "\n",
        "            # Save response log to JSON file\n",
        "            with open(log_filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump(response_log, f, indent=2, default=str, ensure_ascii=False)\n",
        "\n",
        "            logging.info(f\"Response log saved: {os.path.basename(log_filename)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save response log for email {email_index}: {e}\")\n",
        "\n",
        "    def generate_reasoning_chain_optimized(self, email_data):\n",
        "        \"\"\"Optimized reasoning generation for O3-Mini with response logging\"\"\"\n",
        "\n",
        "        subject, body, label, email_index = email_data\n",
        "\n",
        "        try:\n",
        "            # Create optimized prompt\n",
        "            prompt = self.create_optimized_prompt(subject, body, label)\n",
        "            request_start = time.time()\n",
        "\n",
        "            # O3-MINI SPEED OPTIMIZATIONS\n",
        "            response = self.client.responses.create(\n",
        "                model=\"o3-mini\",#o3-mini-20250716\n",
        "                reasoning={\"effort\": self.reasoning_effort},\n",
        "                input=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_output_tokens=self.max_output_tokens\n",
        "            )\n",
        "\n",
        "          #name\n",
        "\n",
        "            request_time = time.time() - request_start\n",
        "\n",
        "            # Save raw API response as JSON log\n",
        "            self._save_response_log(response, email_index, subject, body, label, request_time, prompt)\n",
        "\n",
        "            # Extract response\n",
        "            reasoning_chain = response.output_text if hasattr(response, 'output_text') else \"\"\n",
        "\n",
        "            # Parse prediction\n",
        "            predicted_class_name = self.extract_prediction_fast(reasoning_chain)\n",
        "            predicted_label = 1 if predicted_class_name == \"PHISHING\" else 0 if predicted_class_name == \"LEGITIMATE\" else -1\n",
        "\n",
        "            # Extract token usage\n",
        "            usage_data = self._extract_token_usage(response)\n",
        "\n",
        "            # Thread-safe updates\n",
        "            with self.lock:\n",
        "                self.total_tokens_used += usage_data['total_tokens']\n",
        "                self.total_processing_time += request_time\n",
        "                self.successful_reasoning += 1\n",
        "\n",
        "            # Extract reasoning steps\n",
        "            reasoning_steps = self.extract_reasoning_steps(reasoning_chain)\n",
        "\n",
        "            # Create stats\n",
        "            stats = {\n",
        "                'email_index': email_index,\n",
        "                'processing_time': request_time,\n",
        "                'status': 'success',\n",
        "                'attempt': 1,\n",
        "                **usage_data,\n",
        "                'output_length': len(reasoning_chain),\n",
        "                'word_count': len(reasoning_chain.split()),\n",
        "                'actual_label': label,\n",
        "                'predicted_label': predicted_label,\n",
        "                'prediction_correct': predicted_class_name == (\"LEGITIMATE\" if label == 0 else \"PHISHING\")\n",
        "            }\n",
        "\n",
        "            # Log progress\n",
        "            actual_class = \"LEGITIMATE\" if label == 0 else \"PHISHING\"\n",
        "            match_status = \"CORRECT\" if predicted_class_name == actual_class else \"INCORRECT\"\n",
        "            logging.info(f\"Email {email_index + 1}: {request_time:.2f}s, {usage_data['total_tokens']} tokens, \"\n",
        "                        f\"Actual: {actual_class}, Predicted: {predicted_class_name} ({match_status})\")\n",
        "\n",
        "            return reasoning_chain, reasoning_steps, stats, predicted_class_name, predicted_label\n",
        "\n",
        "        except Exception as e:\n",
        "            request_time = time.time() - request_start if 'request_start' in locals() else 0\n",
        "\n",
        "            with self.lock:\n",
        "                self.failed_reasoning += 1\n",
        "\n",
        "            logging.error(f\"Email {email_index + 1}: {str(e)}\")\n",
        "\n",
        "            # Save error log to response logs directory\n",
        "            try:\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
        "                error_log_filename = os.path.join(self.response_logs_path, f\"error_email_{email_index:05d}_{timestamp}.json\")\n",
        "\n",
        "                error_log = {\n",
        "                    'email_metadata': {\n",
        "                        'email_index': email_index,\n",
        "                        'subject_preview': subject[:100] + '...' if len(subject) > 100 else subject,\n",
        "                        'body_length': len(body),\n",
        "                        'actual_label': label,\n",
        "                        'actual_label_name': \"LEGITIMATE\" if label == 0 else \"PHISHING\",\n",
        "                        'processing_timestamp': datetime.now().isoformat(),\n",
        "                        'request_time_seconds': request_time\n",
        "                    },\n",
        "                    'error_details': {\n",
        "                        'error_message': str(e),\n",
        "                        'error_type': type(e).__name__,\n",
        "                        'status': 'api_error'\n",
        "                    },\n",
        "                    'api_request': {\n",
        "                        'model': 'o3-mini',\n",
        "                        'reasoning_effort': self.reasoning_effort,\n",
        "                        'max_output_tokens': self.max_output_tokens,\n",
        "                        'prompt_length': len(prompt) if 'prompt' in locals() else 0\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                with open(error_log_filename, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(error_log, f, indent=2, default=str, ensure_ascii=False)\n",
        "\n",
        "            except Exception as log_error:\n",
        "                logging.warning(f\"Could not save error log: {log_error}\")\n",
        "\n",
        "            stats = {\n",
        "                'email_index': email_index,\n",
        "                'processing_time': request_time,\n",
        "                'status': 'error',\n",
        "                'error': str(e),\n",
        "                'attempt': 1,\n",
        "                'actual_label': label,\n",
        "                'predicted_label': -1,\n",
        "                'prediction_correct': False\n",
        "            }\n",
        "\n",
        "            return \"\", {}, stats, \"ERROR\", -1\n",
        "\n",
        "    def extract_prediction_fast(self, reasoning_text):\n",
        "        \"\"\"Fast prediction extraction\"\"\"\n",
        "        reasoning_upper = reasoning_text.upper()\n",
        "\n",
        "        # Quick pattern matching\n",
        "        if \"CLASSIFICATION: PHISHING\" in reasoning_upper:\n",
        "            return \"PHISHING\"\n",
        "        elif \"CLASSIFICATION: LEGITIMATE\" in reasoning_upper:\n",
        "            return \"LEGITIMATE\"\n",
        "\n",
        "        # Fallback patterns\n",
        "        patterns = [\n",
        "            r\"I CLASSIFY THIS EMAIL AS:\\s*(\\w+)\",   #I classify this email as\n",
        "            r\"CLASSIFY THIS EMAIL AS:\\s*(\\w+)\",\n",
        "            r\"CLASSIFICATION:\\s*(\\w+)\",\n",
        "            r\"FINAL CLASSIFICATION.*?:\\s*(\\w+)\",\n",
        "            r\"EMAIL AS:\\s*(\\w+)\"\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, reasoning_upper)\n",
        "            if match:\n",
        "                prediction = match.group(1).strip()\n",
        "                if prediction in [\"LEGITIMATE\", \"PHISHING\"]:\n",
        "                    return prediction\n",
        "\n",
        "        return \"UNCERTAIN\"\n",
        "\n",
        "    def extract_reasoning_steps(self, reasoning_text):\n",
        "        \"\"\"Extract reasoning steps from response\"\"\"\n",
        "        steps = {\n",
        "            'sender_analysis': '',\n",
        "            'language_patterns': '',\n",
        "            'social_engineering': '',\n",
        "            'technical_indicators': '',\n",
        "            'risk_assessment': '',\n",
        "            'confidence_assessment': ''\n",
        "        }\n",
        "\n",
        "        if not reasoning_text:\n",
        "            return steps\n",
        "\n",
        "        # Simple extraction based on section headers\n",
        "        sections = [\n",
        "            ('sender_analysis', ['1. SENDER ANALYSIS', 'SENDER ANALYSIS']),\n",
        "            ('language_patterns', ['2. LANGUAGE PATTERNS', 'LANGUAGE PATTERNS']),\n",
        "            ('social_engineering', ['3. SOCIAL ENGINEERING', 'SOCIAL ENGINEERING']),\n",
        "            ('technical_indicators', ['4. TECHNICAL INDICATORS', 'TECHNICAL INDICATORS']),\n",
        "            ('risk_assessment', ['5. RISK ASSESSMENT', 'RISK ASSESSMENT']),\n",
        "            ('confidence_assessment', ['CONFIDENCE:', 'My confidence level'])\n",
        "        ]\n",
        "\n",
        "        text_upper = reasoning_text.upper()\n",
        "\n",
        "        for step_key, markers in sections:\n",
        "            for marker in markers:\n",
        "                pos = text_upper.find(marker.upper())\n",
        "                if pos != -1:\n",
        "                    # Find content after marker\n",
        "                    content_start = pos + len(marker)\n",
        "\n",
        "                    # Find next section or end\n",
        "                    next_pos = len(reasoning_text)\n",
        "                    for next_step_key, next_markers in sections:\n",
        "                        if next_step_key != step_key:\n",
        "                            for next_marker in next_markers:\n",
        "                                next_marker_pos = text_upper.find(next_marker.upper(), content_start)\n",
        "                                if next_marker_pos != -1 and next_marker_pos < next_pos:\n",
        "                                    next_pos = next_marker_pos\n",
        "\n",
        "                    content = reasoning_text[content_start:next_pos].strip()\n",
        "                    if content:\n",
        "                        steps[step_key] = self.clean_text_for_csv(content)\n",
        "                    break\n",
        "\n",
        "        return steps\n",
        "\n",
        "    def clean_text_for_csv(self, text):\n",
        "        \"\"\"Clean text for CSV compatibility\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return str(text)\n",
        "\n",
        "        # Character replacements\n",
        "        replacements = {\n",
        "            '\"': '\"', '\"': '\"', ''': \"'\", ''': \"'\",\n",
        "            '–': '-', '—': '-', '…': '...', '•': '*',\n",
        "            '™': 'TM', '®': '(R)', '©': '(C)'\n",
        "        }\n",
        "\n",
        "        #\n",
        "\n",
        "        for old, new in replacements.items():\n",
        "            text = text.replace(old, new)\n",
        "\n",
        "        # Replace problematic characters\n",
        "        text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "        text = text.replace('\"', \"'\").replace('\\\\', '/').replace('|', ' ')\n",
        "\n",
        "        # Remove non-ASCII characters\n",
        "        text = ''.join(char if ord(char) < 128 else ' ' for char in text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def _extract_token_usage(self, response):\n",
        "        \"\"\"Extract token usage information\"\"\"\n",
        "        usage_data = {\n",
        "            'input_tokens': 0,\n",
        "            'output_tokens': 0,\n",
        "            'total_tokens': 0,\n",
        "            'reasoning_tokens': 0,\n",
        "            'visible_tokens': 0,\n",
        "            'reasoning_ratio': 0\n",
        "        }\n",
        "\n",
        "        if hasattr(response, 'usage') and response.usage:\n",
        "            usage_data['input_tokens'] = getattr(response.usage, 'input_tokens', 0)\n",
        "            usage_data['output_tokens'] = getattr(response.usage, 'output_tokens', 0)\n",
        "            usage_data['total_tokens'] = getattr(response.usage, 'total_tokens', 0)\n",
        "\n",
        "            if hasattr(response.usage, 'output_tokens_details'):\n",
        "                details = response.usage.output_tokens_details\n",
        "                if hasattr(details, 'reasoning_tokens'):\n",
        "                    usage_data['reasoning_tokens'] = details.reasoning_tokens or 0\n",
        "\n",
        "            usage_data['visible_tokens'] = usage_data['output_tokens'] - usage_data['reasoning_tokens']\n",
        "            if usage_data['output_tokens'] > 0:\n",
        "                usage_data['reasoning_ratio'] = usage_data['reasoning_tokens'] / usage_data['output_tokens'] * 100\n",
        "\n",
        "        return usage_data\n",
        "\n",
        "    def load_and_sample_dataset(self, min_words=15, ensure_quality=True):\n",
        "        \"\"\"Load and sample high-quality dataset with enhanced selection\"\"\"\n",
        "        logging.info(\"Loading dataset...\")\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(self.dataset_path, encoding='utf-8', encoding_errors='replace')\n",
        "            logging.info(f\"Loaded dataset with {len(df)} rows\")\n",
        "\n",
        "            if ensure_quality:\n",
        "                # Enhanced quality filtering\n",
        "                logging.info(\"Applying quality filters...\")\n",
        "\n",
        "                # Calculate total word count\n",
        "                df['total_word_count'] = (\n",
        "                    df['subject'].fillna('').astype(str).apply(lambda x: len(x.split())) +\n",
        "                    df['body'].fillna('').astype(str).apply(lambda x: len(x.split()))\n",
        "                )\n",
        "\n",
        "                # Apply minimum word filter\n",
        "                df = df[df['total_word_count'] >= min_words].copy()\n",
        "                logging.info(f\"After word count filter: {len(df)} emails remain\")\n",
        "\n",
        "                # Remove emails with empty or very short bodies\n",
        "                df = df[df['body'].notna() & (df['body'].str.strip() != '')]\n",
        "                df = df[df['body'].str.len() >= 50]  # At least 50 characters\n",
        "                logging.info(f\"After body length filter: {len(df)} emails remain\")\n",
        "\n",
        "                # Remove duplicate content\n",
        "                df = df.drop_duplicates(subset=['subject', 'body'], keep='first')\n",
        "                logging.info(f\"After removing duplicates: {len(df)} emails remain\")\n",
        "\n",
        "                # Remove emails with mostly non-ASCII characters\n",
        "                def has_enough_ascii(text):\n",
        "                    if not text or len(text) == 0:\n",
        "                        return False\n",
        "                    ascii_chars = sum(1 for char in text if ord(char) < 128)\n",
        "                    return ascii_chars / len(text) >= 0.8\n",
        "\n",
        "                df = df[df['body'].apply(has_enough_ascii)]\n",
        "                logging.info(f\"After ASCII filter: {len(df)} emails remain\")\n",
        "\n",
        "                # Ensure variety in email lengths\n",
        "                df['body_length'] = df['body'].str.len()\n",
        "\n",
        "                # Drop temporary columns\n",
        "                df = df.drop(['total_word_count', 'body_length'], axis=1)\n",
        "\n",
        "            # Check class distribution\n",
        "            label_counts = df['label'].value_counts()\n",
        "            logging.info(f\"Label distribution: {dict(label_counts)}\")\n",
        "\n",
        "            # Enhanced sampling for better quality\n",
        "            sampled_data = []\n",
        "\n",
        "            for label in [0, 1]:\n",
        "                label_data = df[df['label'] == label]\n",
        "                available = len(label_data)\n",
        "\n",
        "                if available < self.samples_per_class:\n",
        "                    logging.warning(f\"Only {available} samples available for label {label} (requested: {self.samples_per_class})\")\n",
        "                    sample_size = available\n",
        "                else:\n",
        "                    sample_size = self.samples_per_class\n",
        "\n",
        "                # Stratified sampling by email length for diversity\n",
        "                if sample_size > 100:\n",
        "                    # Sort by body length and take samples from different quartiles\n",
        "                    label_data_sorted = label_data.copy()\n",
        "                    label_data_sorted['body_len'] = label_data_sorted['body'].str.len()\n",
        "                    label_data_sorted = label_data_sorted.sort_values('body_len')\n",
        "\n",
        "                    # Sample from different length quartiles\n",
        "                    quartile_size = sample_size // 4\n",
        "                    remainder = sample_size % 4\n",
        "\n",
        "                    samples = []\n",
        "                    for i in range(4):\n",
        "                        start_idx = i * len(label_data_sorted) // 4\n",
        "                        end_idx = (i + 1) * len(label_data_sorted) // 4\n",
        "                        quartile_data = label_data_sorted.iloc[start_idx:end_idx]\n",
        "\n",
        "                        q_sample_size = quartile_size + (1 if i < remainder else 0)\n",
        "                        if len(quartile_data) > 0:\n",
        "                            if len(quartile_data) >= q_sample_size:\n",
        "                                q_sample = quartile_data.sample(n=q_sample_size, random_state=42)\n",
        "                            else:\n",
        "                                q_sample = quartile_data\n",
        "                            samples.append(q_sample)\n",
        "\n",
        "                    sampled = pd.concat(samples, ignore_index=True)\n",
        "                    sampled = sampled.drop('body_len', axis=1)\n",
        "                else:\n",
        "                    # Simple random sampling for small datasets\n",
        "                    sampled = label_data.sample(n=sample_size, random_state=42)\n",
        "\n",
        "                sampled_data.append(sampled)\n",
        "\n",
        "                label_name = \"Legitimate\" if label == 0 else \"Phishing\"\n",
        "                logging.info(f\"Sampled {len(sampled)} high-quality emails from {label_name} class\")\n",
        "\n",
        "            # Combine and shuffle\n",
        "            self.sample_dataset = pd.concat(sampled_data, ignore_index=True)\n",
        "            self.sample_dataset = self.sample_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "            # Update total samples\n",
        "            self.total_samples = len(self.sample_dataset)\n",
        "\n",
        "            logging.info(f\"Dataset ready with {self.total_samples} high-quality emails\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading dataset: {e}\")\n",
        "            return False\n",
        "\n",
        "    def process_email_batch_concurrent(self, email_batch):\n",
        "        \"\"\"Process batch of emails concurrently\"\"\"\n",
        "\n",
        "        # Prepare email data tuples\n",
        "        email_data_list = []\n",
        "        for idx, row in email_batch.iterrows():\n",
        "            subject = self.clean_text_for_csv(str(row.get('subject', '[No Subject]')))\n",
        "            body = self.clean_text_for_csv(str(row.get('body', '[No Body]')))\n",
        "            label = int(row.get('label', 0))\n",
        "            email_data_list.append((subject, body, label, idx))\n",
        "\n",
        "        # Submit concurrent tasks\n",
        "        futures = []\n",
        "        for email_data in email_data_list:\n",
        "            future = self.executor.submit(self.generate_reasoning_chain_optimized, email_data)\n",
        "            futures.append((future, email_data))\n",
        "\n",
        "        # Collect results\n",
        "        results = []\n",
        "        for future, email_data in futures:\n",
        "            try:\n",
        "                result = future.result(timeout=120)\n",
        "                results.append((result, email_data))\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Concurrent processing error: {e}\")\n",
        "                # Create error result\n",
        "                error_stats = {\n",
        "                    'email_index': email_data[3],\n",
        "                    'processing_time': 0,\n",
        "                    'status': 'timeout_error',\n",
        "                    'error': str(e),\n",
        "                    'actual_label': email_data[2],\n",
        "                    'predicted_label': -1,\n",
        "                    'prediction_correct': False\n",
        "                }\n",
        "                results.append(((\"\", {}, error_stats, \"ERROR\", -1), email_data))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_dataset_optimized(self):\n",
        "        \"\"\"Main processing function with O3-Mini optimizations\"\"\"\n",
        "        logging.info(\"Starting O3-Mini optimized reasoning chain generation...\")\n",
        "\n",
        "        self.start_time = datetime.now()\n",
        "\n",
        "        # Process in batches for concurrent execution\n",
        "        batch_size = self.max_concurrent\n",
        "        total_batches = (len(self.sample_dataset) + batch_size - 1) // batch_size\n",
        "\n",
        "        # Progress tracking\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        # Create progress bar\n",
        "        pbar = tqdm(total=len(self.sample_dataset), desc=\"Processing emails with O3-Mini\")\n",
        "\n",
        "        # Process batches\n",
        "        for batch_idx in range(total_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min(start_idx + batch_size, len(self.sample_dataset))\n",
        "\n",
        "            batch_df = self.sample_dataset.iloc[start_idx:end_idx]\n",
        "\n",
        "            logging.info(f\"Processing batch {batch_idx + 1}/{total_batches} ({len(batch_df)} emails)\")\n",
        "\n",
        "            # Process batch concurrently\n",
        "            batch_results = self.process_email_batch_concurrent(batch_df)\n",
        "\n",
        "            # Process results\n",
        "            for (reasoning_chain, reasoning_steps, stats, predicted_class_name, predicted_label), email_data in batch_results:\n",
        "                email_index = email_data[3]\n",
        "                row = self.sample_dataset.iloc[email_index]\n",
        "\n",
        "                # Update tracking\n",
        "                if predicted_label != -1:\n",
        "                    total_predictions += 1\n",
        "                    if predicted_label == int(row.get('label', 0)):\n",
        "                        correct_predictions += 1\n",
        "\n",
        "                # Create enhanced record with full reasoning steps\n",
        "                enhanced_record = {\n",
        "                    'original_index': email_index,\n",
        "                    'subject': self.clean_text_for_csv(str(row.get('subject', ''))),\n",
        "                    'body': self.clean_text_for_csv(str(row.get('body', ''))),\n",
        "                    'original_label': int(row.get('label', 0)),\n",
        "                    'original_label_name': \"LEGITIMATE\" if int(row.get('label', 0)) == 0 else \"PHISHING\",\n",
        "                    'model_predicted_label': predicted_label,\n",
        "                    'model_predicted_label_name': predicted_class_name,\n",
        "                    'prediction_correct': predicted_label == int(row.get('label', 0)),\n",
        "                    'reasoning_chain': self.clean_text_for_csv(reasoning_chain),\n",
        "                    'sender_analysis': reasoning_steps.get('sender_analysis', ''),\n",
        "                    'language_patterns': reasoning_steps.get('language_patterns', ''),\n",
        "                    'social_engineering': reasoning_steps.get('social_engineering', ''),\n",
        "                    'technical_indicators': reasoning_steps.get('technical_indicators', ''),\n",
        "                    'risk_assessment': reasoning_steps.get('risk_assessment', ''),\n",
        "                    'confidence_assessment': reasoning_steps.get('confidence_assessment', ''),\n",
        "                    'reasoning_generated': predicted_label != -1,\n",
        "                    'reasoning_length': len(reasoning_chain),\n",
        "                    'reasoning_word_count': len(reasoning_chain.split()),\n",
        "                    'processing_time': stats['processing_time'],\n",
        "                    'tokens_used': stats.get('total_tokens', 0),\n",
        "                    'reasoning_tokens': stats.get('reasoning_tokens', 0),\n",
        "                    'o3_mini_effort': self.reasoning_effort,\n",
        "                    'generation_timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "\n",
        "                self.enhanced_dataset.append(enhanced_record)\n",
        "                self.processing_stats.append(stats)\n",
        "                self.processed_count += 1\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.update(1)\n",
        "\n",
        "            # Progress update\n",
        "            accuracy = (correct_predictions / total_predictions * 100) if total_predictions > 0 else 0\n",
        "            elapsed = (datetime.now() - self.start_time).total_seconds()\n",
        "            emails_per_sec = self.processed_count / elapsed if elapsed > 0 else 0\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Accuracy': f\"{accuracy:.1f}%\",\n",
        "                'Speed': f\"{emails_per_sec:.1f} emails/sec\",\n",
        "                'Success': f\"{self.successful_reasoning}/{self.processed_count}\"\n",
        "            })\n",
        "\n",
        "            # Save checkpoint periodically\n",
        "            if self.processed_count % self.checkpoint_interval == 0:\n",
        "                self._save_checkpoint()\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        # Final save and summary\n",
        "        self._save_checkpoint(force=True)\n",
        "        self._save_final_results()\n",
        "        self._print_summary()\n",
        "\n",
        "        # Cleanup\n",
        "        self.executor.shutdown(wait=True)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _save_checkpoint(self, force=False):\n",
        "        \"\"\"Save checkpoint\"\"\"\n",
        "        if not force and self.processed_count % self.checkpoint_interval != 0:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            checkpoint_file = os.path.join(self.checkpoint_path, f'o3_mini_checkpoint_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl')\n",
        "\n",
        "            checkpoint_data = {\n",
        "                'processed_count': self.processed_count,\n",
        "                'successful_reasoning': self.successful_reasoning,\n",
        "                'failed_reasoning': self.failed_reasoning,\n",
        "                'total_tokens_used': self.total_tokens_used,\n",
        "                'total_processing_time': self.total_processing_time,\n",
        "                'enhanced_dataset': self.enhanced_dataset,\n",
        "                'processing_stats': self.processing_stats,\n",
        "                'start_time': self.start_time,\n",
        "                'o3_mini_config': {\n",
        "                    'reasoning_effort': self.reasoning_effort,\n",
        "                    'max_output_tokens': self.max_output_tokens,\n",
        "                    'max_concurrent': self.max_concurrent,\n",
        "                    'speed_mode': self.o3_speed_mode\n",
        "                },\n",
        "                'timestamp': datetime.now()\n",
        "            }\n",
        "\n",
        "            with open(checkpoint_file, 'wb') as f:\n",
        "                pickle.dump(checkpoint_data, f)\n",
        "\n",
        "            # Also save as CSV for inspection\n",
        "            if self.enhanced_dataset:\n",
        "                csv_file = checkpoint_file.replace('.pkl', '.csv')\n",
        "                pd.DataFrame(self.enhanced_dataset).to_csv(csv_file, index=False, encoding='utf-8')\n",
        "\n",
        "            logging.info(f\"Checkpoint saved: {checkpoint_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save checkpoint: {e}\")\n",
        "\n",
        "    def _save_final_results(self):\n",
        "        \"\"\"Save final results\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        try:\n",
        "            # Save enhanced dataset\n",
        "            enhanced_df = pd.DataFrame(self.enhanced_dataset)\n",
        "            enhanced_filename = os.path.join(self.final_dataset_path, f\"o3_mini_optimized_dataset_{timestamp}.csv\")\n",
        "            enhanced_df.to_csv(enhanced_filename, index=False, encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "            logging.info(f\"Enhanced dataset saved: {enhanced_filename}\")\n",
        "\n",
        "            # Save processing stats\n",
        "            stats_filename = os.path.join(self.final_dataset_path, f\"processing_stats_{timestamp}.json\")\n",
        "            with open(stats_filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump({\n",
        "                    'pipeline_summary': self._get_pipeline_summary(),\n",
        "                    'processing_stats': self.processing_stats\n",
        "                }, f, indent=2, default=str)\n",
        "            logging.info(f\"Processing stats saved: {stats_filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving results: {e}\")\n",
        "\n",
        "    def _get_pipeline_summary(self):\n",
        "        \"\"\"Get pipeline execution summary\"\"\"\n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - self.start_time).total_seconds() if self.start_time else 0\n",
        "\n",
        "        correct = sum(1 for r in self.enhanced_dataset if r.get('prediction_correct', False))\n",
        "        total_pred = sum(1 for r in self.enhanced_dataset if r.get('model_predicted_label', -1) != -1)\n",
        "        accuracy = (correct / total_pred * 100) if total_pred > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'start_time': self.start_time.isoformat() if self.start_time else None,\n",
        "            'end_time': end_time.isoformat(),\n",
        "            'total_duration_seconds': duration,\n",
        "            'total_duration_formatted': str(timedelta(seconds=int(duration))),\n",
        "            'total_emails_processed': self.processed_count,\n",
        "            'successful_reasoning': self.successful_reasoning,\n",
        "            'failed_reasoning': self.failed_reasoning,\n",
        "            'success_rate': (self.successful_reasoning / self.processed_count * 100) if self.processed_count > 0 else 0,\n",
        "            'prediction_accuracy': accuracy,\n",
        "            'correct_predictions': correct,\n",
        "            'total_predictions': total_pred,\n",
        "            'total_tokens_used': self.total_tokens_used,\n",
        "            'average_time_per_email': duration / self.processed_count if self.processed_count > 0 else 0,\n",
        "            'average_tokens_per_email': self.total_tokens_used / self.processed_count if self.processed_count > 0 else 0,\n",
        "            'o3_mini_configuration': {\n",
        "                'reasoning_effort': self.reasoning_effort,\n",
        "                'max_output_tokens': self.max_output_tokens,\n",
        "                'max_concurrent': self.max_concurrent,\n",
        "                'speed_mode': self.o3_speed_mode\n",
        "            },\n",
        "            'samples_per_class': self.samples_per_class,\n",
        "            'google_drive_enabled': self.use_gdrive,\n",
        "            'checkpoint_interval': self.checkpoint_interval\n",
        "        }\n",
        "\n",
        "    def _print_summary(self):\n",
        "        \"\"\"Print execution summary\"\"\"\n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - self.start_time).total_seconds()\n",
        "\n",
        "        correct = sum(1 for r in self.enhanced_dataset if r.get('prediction_correct', False))\n",
        "        total_pred = sum(1 for r in self.enhanced_dataset if r.get('model_predicted_label', -1) != -1)\n",
        "        accuracy = (correct / total_pred * 100) if total_pred > 0 else 0\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"O3-MINI OPTIMIZED REASONING PIPELINE COMPLETE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nO3-MINI CONFIGURATION:\")\n",
        "        print(f\"  Model: o3-mini ({self.reasoning_effort} effort)\")\n",
        "        print(f\"  Max tokens: {self.max_output_tokens:,}\")\n",
        "        print(f\"  Concurrent requests: {self.max_concurrent}\")\n",
        "        print(f\"  Speed mode: {self.o3_speed_mode}\")\n",
        "\n",
        "        print(f\"\\nPROCESSING SUMMARY:\")\n",
        "        print(f\"  Total Emails: {self.processed_count:,}\")\n",
        "        print(f\"  Successful: {self.successful_reasoning:,}\")\n",
        "        print(f\"  Failed: {self.failed_reasoning:,}\")\n",
        "        print(f\"  Success Rate: {(self.successful_reasoning/self.processed_count*100):.1f}%\")\n",
        "        print(f\"  Prediction Accuracy: {accuracy:.1f}%\")\n",
        "\n",
        "        print(f\"\\nPERFORMANCE:\")\n",
        "        print(f\"  Total Duration: {timedelta(seconds=int(duration))}\")\n",
        "        print(f\"  Avg Time/Email: {duration/self.processed_count:.2f}s\")\n",
        "        print(f\"  Emails/Second: {self.processed_count/duration:.2f}\")\n",
        "\n",
        "        print(f\"\\nRESOURCE USAGE:\")\n",
        "        print(f\"  Total Tokens: {self.total_tokens_used:,}\")\n",
        "        print(f\"  Avg Tokens/Email: {self.total_tokens_used/self.processed_count:.0f}\")\n",
        "\n",
        "        print(f\"\\nOUTPUT LOCATION:\")\n",
        "        print(f\"  Directory: {self.output_dir}\")\n",
        "        print(f\"  Google Drive: {'Enabled' if self.use_gdrive else 'Disabled'}\")\n",
        "\n",
        "        print(f\"\\nFILES CREATED:\")\n",
        "        print(f\"  Enhanced Dataset: final_datasets/o3_mini_optimized_dataset_*.csv\")\n",
        "        print(f\"  Processing Stats: final_datasets/processing_stats_*.json\")\n",
        "        print(f\"  Response Logs: response_logs/response_email_*.json\")\n",
        "        print(f\"  Checkpoints: checkpoints/o3_mini_checkpoint_*.pkl\")\n",
        "        print(f\"  Pipeline Logs: logs/o3_mini_optimized_*.log\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# ==================================================\n",
        "# O3-MINI OPTIMIZED GOOGLE COLAB FUNCTIONS\n",
        "# ==================================================\n",
        "\n",
        "def run_o3_mini_speed_test(samples_per_class=10, speed_mode=\"medium\"):\n",
        "    \"\"\"Test the O3-Mini speed optimizations\"\"\"\n",
        "    print(\"O3-MINI SPEED TEST MODE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Speed mode: {speed_mode}\")\n",
        "    print(f\"Testing with: {samples_per_class * 2} emails\")\n",
        "\n",
        "    try:\n",
        "        pipeline = O3MiniOptimizedPipeline(\n",
        "            dataset_path=\"Enron.csv\",\n",
        "            samples_per_class=samples_per_class,\n",
        "            o3_speed_mode=speed_mode,\n",
        "            max_concurrent=8\n",
        "        )\n",
        "\n",
        "        if not pipeline.load_and_sample_dataset():\n",
        "            return False\n",
        "\n",
        "        success = pipeline.process_dataset_optimized()\n",
        "        return success\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"O3-Mini speed test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def run_o3_mini_optimized_pipeline(samples_per_class=5000,\n",
        "                                  speed_mode=\"medium\",\n",
        "                                  max_concurrent=8,\n",
        "                                  api_key=None):\n",
        "    \"\"\"Run the full O3-Mini optimized pipeline\"\"\"\n",
        "\n",
        "    print(f\"O3-MINI OPTIMIZED PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Speed mode: {speed_mode}\")\n",
        "    print(f\"Max concurrent: {max_concurrent}\")\n",
        "    print(f\"Total emails: {samples_per_class * 2:,}\")\n",
        "\n",
        "    try:\n",
        "        pipeline = O3MiniOptimizedPipeline(\n",
        "            samples_per_class=samples_per_class,\n",
        "            o3_speed_mode=speed_mode,\n",
        "            max_concurrent=max_concurrent,\n",
        "            api_key=api_key\n",
        "        )\n",
        "\n",
        "        if not pipeline.load_and_sample_dataset():\n",
        "            return False\n",
        "\n",
        "        success = pipeline.process_dataset_optimized()\n",
        "        return success\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"O3-Mini pipeline failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def show_o3_mini_speed_options():\n",
        "    \"\"\"Show O3-Mini speed optimization options\"\"\"\n",
        "    print(\"O3-MINI SPEED OPTIMIZATION OPTIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\"\"\n",
        "O3-MINI SPEED MODES:\n",
        "\n",
        "LOW EFFORT MODE (Maximum Speed):\n",
        "```python\n",
        "run_o3_mini_optimized_pipeline(\n",
        "    samples_per_class=5000,\n",
        "    speed_mode=\"low\",\n",
        "    max_concurrent=8\n",
        ")\n",
        "```\n",
        "- Reasoning: o3-mini with LOW effort\n",
        "- Processing: Fastest available mode\n",
        "- Accuracy: Good performance maintained\n",
        "- Cost: Significant reduction\n",
        "\n",
        "MEDIUM EFFORT MODE (Recommended):\n",
        "```python\n",
        "run_o3_mini_optimized_pipeline(\n",
        "    samples_per_class=5000,\n",
        "    speed_mode=\"medium\",\n",
        "    max_concurrent=8\n",
        ")\n",
        "```\n",
        "- Reasoning: o3-mini with MEDIUM effort\n",
        "- Processing: Balanced speed and accuracy\n",
        "- Accuracy: High quality results\n",
        "- Cost: Moderate reduction\n",
        "\n",
        "HIGH EFFORT MODE (Best Quality):\n",
        "```python\n",
        "run_o3_mini_optimized_pipeline(\n",
        "    samples_per_class=5000,\n",
        "    speed_mode=\"high\",\n",
        "    max_concurrent=6\n",
        ")\n",
        "```\n",
        "- Reasoning: o3-mini with HIGH effort\n",
        "- Processing: Enhanced quality analysis\n",
        "- Accuracy: Maximum available\n",
        "- Cost: Optimized from original\n",
        "\n",
        "QUICK TEST:\n",
        "```python\n",
        "run_o3_mini_speed_test(samples_per_class=10, speed_mode=\"medium\")\n",
        "```\n",
        "\n",
        "CONCURRENT PROCESSING OPTIONS:\n",
        "- max_concurrent=8: Recommended for most accounts\n",
        "- max_concurrent=6: Conservative approach\n",
        "- max_concurrent=10: Aggressive processing\n",
        "\n",
        "OPTIMIZATION FEATURES:\n",
        "- Optimized prompts for efficiency\n",
        "- Reduced token limits for speed\n",
        "- Removed artificial delays\n",
        "- Concurrent processing capabilities\n",
        "- Smart batching and error handling\n",
        "- Complete response logging system\n",
        "- Enhanced email quality selection\n",
        "- Real-time progress tracking\n",
        "- Google Drive integration maintained\n",
        "\"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"O3-Mini Optimized Reasoning Pipeline\")\n",
        "    print(\"Use these functions in your notebook:\")\n",
        "    print(\"- run_o3_mini_speed_test(10)\")\n",
        "    print(\"- run_o3_mini_optimized_pipeline(5000)\")\n",
        "    print(\"- show_o3_mini_speed_options()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbOy6kMzaeVL"
      },
      "outputs": [],
      "source": [
        "# Test with 20 emails\n",
        "run_o3_mini_speed_test(samples_per_class=2, speed_mode=\"medium\")  #reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SoCgcx90eR-x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Full pipeline with 10,000 emails\n",
        "run_o3_mini_optimized_pipeline(\n",
        "    samples_per_class=5000,\n",
        "    speed_mode=\"medium\",  # or \"low\" for max speed #high\n",
        "    max_concurrent=8\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Analysis"
      ],
      "metadata": {
        "id": "V3iFhf2Sx__2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib seaborn plotly wordcloud nltk textstat textblob scikit-learn"
      ],
      "metadata": {
        "id": "D0Q9jXoLyV5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "COMPLETE O3-MINI PHISHING DETECTION DATASET ANALYSIS\n",
        "Final Version - Ready to Run in Google Colab\n",
        "\n",
        "Research: Enhancing GPT-4o's Phishing Detection with o3-mini–Generated Chain-of-Thought Augmentation\n",
        "Dataset: /content/drive/MyDrive/phishing_detection_final/output/final_datasets/o3_mini_optimized_dataset_20250709_185906.csv\n",
        "\n",
        "Usage: Simply run all cells in order, or run the complete analysis with run_complete_analysis()\n",
        "\"\"\"\n",
        "\n",
        "# ===========================\n",
        "# STEP 1: INSTALL DEPENDENCIES\n",
        "# ===========================\n",
        "\n",
        "# Install required packages\n",
        "!pip install wordcloud textstat textblob plotly -q\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✅ Google Drive mounted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Drive mounting issue: {e}\")\n",
        "\n",
        "# ===========================\n",
        "# STEP 2: IMPORT LIBRARIES\n",
        "# ===========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text analysis libraries\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import os\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# NLTK setup\n",
        "import nltk\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except:\n",
        "    stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
        "\n",
        "# Additional libraries\n",
        "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"🔧 All libraries imported successfully!\")\n",
        "\n",
        "# ===========================\n",
        "# STEP 3: DATASET CONFIGURATION\n",
        "# ===========================\n",
        "\n",
        "# Your specific dataset path\n",
        "DATASET_PATH = \"/content/drive/MyDrive/phishing_detection_final/output/final_datasets/o3_mini_optimized_dataset_20250709_185906.csv\"\n",
        "\n",
        "print(f\"📂 Target dataset: {DATASET_PATH}\")\n",
        "\n",
        "# ===========================\n",
        "# STEP 4: MAIN ANALYSIS CLASS\n",
        "# ===========================\n",
        "\n",
        "class O3MiniDatasetAnalyzer:\n",
        "    \"\"\"Complete analyzer for o3-mini generated phishing detection dataset\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.df = None\n",
        "        self.analysis_results = {}\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"Load and validate the dataset\"\"\"\n",
        "        print(\"\\n📊 LOADING O3-MINI DATASET\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if not os.path.exists(self.dataset_path):\n",
        "            print(f\"❌ Dataset file not found: {self.dataset_path}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Try multiple encodings\n",
        "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
        "                try:\n",
        "                    self.df = pd.read_csv(self.dataset_path, encoding=encoding)\n",
        "                    print(f\"✅ Successfully loaded with {encoding} encoding\")\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if self.df is None:\n",
        "                raise ValueError(\"Could not load dataset with any encoding\")\n",
        "\n",
        "            # Basic info\n",
        "            print(f\"📈 Dataset Shape: {self.df.shape[0]:,} rows × {self.df.shape[1]} columns\")\n",
        "            print(f\"💾 Memory Usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "            print(f\"📅 File Size: {os.path.getsize(self.dataset_path) / 1024**2:.2f} MB\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading dataset: {e}\")\n",
        "            return False\n",
        "\n",
        "    def basic_overview(self):\n",
        "        \"\"\"Display basic dataset overview\"\"\"\n",
        "        print(\"\\n🔍 DATASET OVERVIEW\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Column information\n",
        "        print(\"📋 COLUMNS:\")\n",
        "        for i, col in enumerate(self.df.columns, 1):\n",
        "            dtype = str(self.df[col].dtype)\n",
        "            null_count = self.df[col].isnull().sum()\n",
        "            null_pct = (null_count / len(self.df)) * 100\n",
        "            print(f\"  {i:2d}. {col:<30} | {dtype:<12} | Nulls: {null_count:>4} ({null_pct:>4.1f}%)\")\n",
        "\n",
        "        # Key statistics\n",
        "        print(f\"\\n🎯 KEY STATISTICS:\")\n",
        "\n",
        "        # Class distribution\n",
        "        if 'original_label' in self.df.columns:\n",
        "            label_counts = self.df['original_label'].value_counts()\n",
        "            print(f\"  Class Distribution:\")\n",
        "            print(f\"    Legitimate (0): {label_counts.get(0, 0):,} ({label_counts.get(0, 0)/len(self.df)*100:.1f}%)\")\n",
        "            print(f\"    Phishing (1):   {label_counts.get(1, 0):,} ({label_counts.get(1, 0)/len(self.df)*100:.1f}%)\")\n",
        "\n",
        "        # Model performance\n",
        "        if 'prediction_correct' in self.df.columns:\n",
        "            accuracy = self.df['prediction_correct'].mean() * 100\n",
        "            correct_count = self.df['prediction_correct'].sum()\n",
        "            print(f\"  O3-Mini Accuracy: {accuracy:.2f}% ({correct_count:,}/{len(self.df):,})\")\n",
        "\n",
        "        # Processing efficiency\n",
        "        if 'processing_time' in self.df.columns:\n",
        "            avg_time = self.df['processing_time'].mean()\n",
        "            total_time = self.df['processing_time'].sum()\n",
        "            print(f\"  Processing Time: {avg_time:.2f}s avg, {total_time/3600:.2f}h total\")\n",
        "\n",
        "        if 'tokens_used' in self.df.columns:\n",
        "            avg_tokens = self.df['tokens_used'].mean()\n",
        "            total_tokens = self.df['tokens_used'].sum()\n",
        "            print(f\"  Token Usage: {avg_tokens:.0f} avg, {total_tokens:,} total\")\n",
        "\n",
        "        # Store results\n",
        "        self.analysis_results['basic_stats'] = {\n",
        "            'total_emails': len(self.df),\n",
        "            'columns': len(self.df.columns),\n",
        "            'memory_mb': self.df.memory_usage(deep=True).sum() / 1024**2\n",
        "        }\n",
        "\n",
        "    def analyze_text_characteristics(self):\n",
        "        \"\"\"Analyze text characteristics of emails and reasoning\"\"\"\n",
        "        print(\"\\n📝 TEXT ANALYSIS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        text_analysis = {}\n",
        "\n",
        "        # Analyze key text columns\n",
        "        text_columns = {\n",
        "            'subject': 'Email Subjects',\n",
        "            'body': 'Email Bodies',\n",
        "            'reasoning_chain': 'Reasoning Chains'\n",
        "        }\n",
        "\n",
        "        for col, label in text_columns.items():\n",
        "            if col in self.df.columns:\n",
        "                print(f\"\\n📊 {label.upper()}:\")\n",
        "\n",
        "                texts = self.df[col].fillna('').astype(str)\n",
        "\n",
        "                # Length statistics\n",
        "                char_lengths = texts.str.len()\n",
        "                word_counts = texts.apply(lambda x: len(x.split()))\n",
        "\n",
        "                print(f\"  Character Length - Mean: {char_lengths.mean():.0f}, Median: {char_lengths.median():.0f}\")\n",
        "                print(f\"  Word Count - Mean: {word_counts.mean():.0f}, Median: {word_counts.median():.0f}\")\n",
        "                print(f\"  Range: {char_lengths.min():.0f} - {char_lengths.max():.0f} characters\")\n",
        "\n",
        "                text_analysis[col] = {\n",
        "                    'avg_chars': char_lengths.mean(),\n",
        "                    'avg_words': word_counts.mean(),\n",
        "                    'median_chars': char_lengths.median(),\n",
        "                    'median_words': word_counts.median()\n",
        "                }\n",
        "\n",
        "        # Reasoning component analysis\n",
        "        reasoning_components = [\n",
        "            'sender_analysis', 'language_patterns', 'social_engineering',\n",
        "            'technical_indicators', 'risk_assessment'\n",
        "        ]\n",
        "\n",
        "        print(f\"\\n🧠 REASONING COMPONENTS:\")\n",
        "        component_analysis = {}\n",
        "\n",
        "        for component in reasoning_components:\n",
        "            if component in self.df.columns:\n",
        "                comp_texts = self.df[component].fillna('').astype(str)\n",
        "                non_empty = (comp_texts.str.len() > 10).sum()\n",
        "                completeness = non_empty / len(self.df) * 100\n",
        "                avg_words = comp_texts.apply(lambda x: len(x.split())).mean()\n",
        "\n",
        "                print(f\"  {component.replace('_', ' ').title():<20}: {completeness:>5.1f}% complete, {avg_words:>4.0f} words avg\")\n",
        "\n",
        "                component_analysis[component] = {\n",
        "                    'completeness': completeness,\n",
        "                    'avg_words': avg_words\n",
        "                }\n",
        "\n",
        "        self.analysis_results['text_analysis'] = text_analysis\n",
        "        self.analysis_results['reasoning_components'] = component_analysis\n",
        "\n",
        "    def analyze_model_performance(self):\n",
        "        \"\"\"Detailed model performance analysis\"\"\"\n",
        "        print(\"\\n🤖 O3-MINI MODEL PERFORMANCE\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if not all(col in self.df.columns for col in ['original_label', 'model_predicted_label']):\n",
        "            print(\"❌ Required columns not found for performance analysis\")\n",
        "            return\n",
        "\n",
        "        # Filter valid predictions\n",
        "        valid_mask = self.df['model_predicted_label'].isin([0, 1])\n",
        "        valid_df = self.df[valid_mask].copy()\n",
        "\n",
        "        if len(valid_df) == 0:\n",
        "            print(\"❌ No valid predictions found\")\n",
        "            return\n",
        "\n",
        "        y_true = valid_df['original_label']\n",
        "        y_pred = valid_df['model_predicted_label']\n",
        "\n",
        "        # Overall metrics\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        print(f\"🎯 Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        print(f\"\\n📊 Confusion Matrix:\")\n",
        "        print(f\"                 Predicted\")\n",
        "        print(f\"Actual     Legit    Phishing\")\n",
        "        print(f\"Legit      {cm[0,0]:>5}    {cm[0,1]:>8}\")\n",
        "        print(f\"Phishing   {cm[1,0]:>5}    {cm[1,1]:>8}\")\n",
        "\n",
        "        # Detailed classification report\n",
        "        print(f\"\\n📈 Classification Report:\")\n",
        "        report = classification_report(y_true, y_pred, target_names=['Legitimate', 'Phishing'], output_dict=True)\n",
        "\n",
        "        for class_name, metrics in report.items():\n",
        "            if class_name in ['Legitimate', 'Phishing']:\n",
        "                print(f\"  {class_name}:\")\n",
        "                print(f\"    Precision: {metrics['precision']:.3f}\")\n",
        "                print(f\"    Recall:    {metrics['recall']:.3f}\")\n",
        "                print(f\"    F1-Score:  {metrics['f1-score']:.3f}\")\n",
        "\n",
        "        # Performance by reasoning quality\n",
        "        if 'reasoning_length' in self.df.columns:\n",
        "            print(f\"\\n🧠 Performance by Reasoning Length:\")\n",
        "            valid_df['reasoning_quartile'] = pd.qcut(\n",
        "                valid_df['reasoning_length'],\n",
        "                q=4,\n",
        "                labels=['Q1 (Short)', 'Q2', 'Q3', 'Q4 (Long)']\n",
        "            )\n",
        "\n",
        "            quartile_accuracy = valid_df.groupby('reasoning_quartile')['prediction_correct'].mean()\n",
        "            for quartile, acc in quartile_accuracy.items():\n",
        "                print(f\"    {quartile}: {acc:.3f} ({acc*100:.1f}%)\")\n",
        "\n",
        "        self.analysis_results['model_performance'] = {\n",
        "            'accuracy': accuracy,\n",
        "            'confusion_matrix': cm.tolist(),\n",
        "            'classification_report': report\n",
        "        }\n",
        "\n",
        "    def analyze_processing_efficiency(self):\n",
        "        \"\"\"Analyze processing efficiency and costs\"\"\"\n",
        "        print(\"\\n⚡ PROCESSING EFFICIENCY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Processing time analysis\n",
        "        if 'processing_time' in self.df.columns:\n",
        "            time_stats = self.df['processing_time'].describe()\n",
        "            total_time = self.df['processing_time'].sum()\n",
        "\n",
        "            print(f\"⏱️  Processing Time Statistics:\")\n",
        "            print(f\"  Total Time: {total_time/3600:.2f} hours\")\n",
        "            print(f\"  Average: {time_stats['mean']:.2f}s per email\")\n",
        "            print(f\"  Median: {time_stats['50%']:.2f}s\")\n",
        "            print(f\"  Range: {time_stats['min']:.2f}s - {time_stats['max']:.2f}s\")\n",
        "            print(f\"  Throughput: {len(self.df)/total_time:.2f} emails/second\")\n",
        "\n",
        "        # Token usage and cost analysis\n",
        "        if 'tokens_used' in self.df.columns:\n",
        "            token_stats = self.df['tokens_used'].describe()\n",
        "            total_tokens = self.df['tokens_used'].sum()\n",
        "\n",
        "            # Cost calculation (o3-mini pricing: $1.10 per million input tokens)\n",
        "            # estimated_cost = (total_tokens / 1_000_000) * 1.10\n",
        "            # cost_per_email = estimated_cost / len(self.df)\n",
        "\n",
        "            print(f\"\\n🪙 Token Usage & Cost Analysis:\")\n",
        "            print(f\"  Total Tokens: {total_tokens:,}\")\n",
        "            print(f\"  Average per Email: {token_stats['mean']:.0f}\")\n",
        "            print(f\"  Median: {token_stats['50%']:.0f}\")\n",
        "            print(f\"  Range: {token_stats['min']:.0f} - {token_stats['max']:.0f}\")\n",
        "            # print(f\"\\n💰 Cost Estimation:\")\n",
        "            # print(f\"  Total Cost: ${estimated_cost:.2f}\")\n",
        "            # print(f\"  Cost per Email: ${cost_per_email:.4f}\")\n",
        "            # print(f\"  Cost per 1000 emails: ${cost_per_email * 1000:.2f}\")\n",
        "\n",
        "            self.analysis_results['efficiency'] = {\n",
        "                'total_tokens': int(total_tokens),\n",
        "                'avg_tokens': token_stats['mean']\n",
        "                # 'estimated_cost': estimated_cost,\n",
        "                # 'cost_per_email': cost_per_email\n",
        "            }\n",
        "\n",
        "    def create_visualizations(self):\n",
        "        \"\"\"Create comprehensive visualizations\"\"\"\n",
        "        print(\"\\n📊 CREATING VISUALIZATIONS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Create a comprehensive dashboard\n",
        "        fig = plt.figure(figsize=(20, 24))\n",
        "\n",
        "        # 1. Class Distribution\n",
        "        if 'original_label' in self.df.columns:\n",
        "            plt.subplot(4, 3, 1)\n",
        "            label_counts = self.df['original_label'].value_counts()\n",
        "            colors = ['#2E8B57', '#DC143C']\n",
        "            plt.pie(label_counts.values, labels=['Legitimate', 'Phishing'],\n",
        "                   autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "            plt.title('Class Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "        # 2. Model Accuracy\n",
        "        if 'prediction_correct' in self.df.columns:\n",
        "            plt.subplot(4, 3, 2)\n",
        "            correct_counts = self.df['prediction_correct'].value_counts()\n",
        "            colors = ['#FF6B6B', '#4ECDC4']\n",
        "            plt.pie(correct_counts.values, labels=['Incorrect', 'Correct'],\n",
        "                   autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "            plt.title('Prediction Accuracy', fontsize=14, fontweight='bold')\n",
        "\n",
        "        # 3. Processing Time Distribution\n",
        "        if 'processing_time' in self.df.columns:\n",
        "            plt.subplot(4, 3, 3)\n",
        "            plt.hist(self.df['processing_time'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            plt.xlabel('Processing Time (seconds)')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.title('Processing Time Distribution', fontsize=14, fontweight='bold')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Token Usage Distribution\n",
        "        if 'tokens_used' in self.df.columns:\n",
        "            plt.subplot(4, 3, 4)\n",
        "            plt.hist(self.df['tokens_used'], bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "            plt.xlabel('Tokens Used')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.title('Token Usage Distribution', fontsize=14, fontweight='bold')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. Subject Length by Class\n",
        "        if 'subject' in self.df.columns and 'original_label' in self.df.columns:\n",
        "            plt.subplot(4, 3, 5)\n",
        "            subject_lengths = self.df['subject'].fillna('').str.len()\n",
        "            for label, name, color in [(0, 'Legitimate', 'green'), (1, 'Phishing', 'red')]:\n",
        "                data = subject_lengths[self.df['original_label'] == label]\n",
        "                plt.hist(data, bins=30, alpha=0.6, label=name, color=color, density=True)\n",
        "            plt.xlabel('Subject Length (characters)')\n",
        "            plt.ylabel('Density')\n",
        "            plt.title('Subject Length by Class', fontsize=14, fontweight='bold')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6. Body Word Count by Class\n",
        "        if 'body' in self.df.columns and 'original_label' in self.df.columns:\n",
        "            plt.subplot(4, 3, 6)\n",
        "            body_words = self.df['body'].fillna('').apply(lambda x: len(x.split()))\n",
        "            for label, name, color in [(0, 'Legitimate', 'green'), (1, 'Phishing', 'red')]:\n",
        "                data = body_words[self.df['original_label'] == label]\n",
        "                plt.hist(data, bins=30, alpha=0.6, label=name, color=color, density=True)\n",
        "            plt.xlabel('Body Word Count')\n",
        "            plt.ylabel('Density')\n",
        "            plt.title('Email Body Length by Class', fontsize=14, fontweight='bold')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 7. Reasoning Length Distribution\n",
        "        if 'reasoning_length' in self.df.columns:\n",
        "            plt.subplot(4, 3, 7)\n",
        "            plt.hist(self.df['reasoning_length'], bins=50, alpha=0.7, color='gold', edgecolor='black')\n",
        "            plt.xlabel('Reasoning Length (characters)')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.title('Reasoning Chain Length', fontsize=14, fontweight='bold')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 8. Confusion Matrix Heatmap\n",
        "        if 'original_label' in self.df.columns and 'model_predicted_label' in self.df.columns:\n",
        "            plt.subplot(4, 3, 8)\n",
        "            valid_mask = self.df['model_predicted_label'].isin([0, 1])\n",
        "            if valid_mask.any():\n",
        "                cm = confusion_matrix(\n",
        "                    self.df.loc[valid_mask, 'original_label'],\n",
        "                    self.df.loc[valid_mask, 'model_predicted_label']\n",
        "                )\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                           xticklabels=['Legitimate', 'Phishing'],\n",
        "                           yticklabels=['Legitimate', 'Phishing'])\n",
        "                plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "                plt.ylabel('Actual')\n",
        "                plt.xlabel('Predicted')\n",
        "\n",
        "        # 9. Processing Time vs Tokens\n",
        "        if 'processing_time' in self.df.columns and 'tokens_used' in self.df.columns:\n",
        "            plt.subplot(4, 3, 9)\n",
        "            plt.scatter(self.df['processing_time'], self.df['tokens_used'],\n",
        "                       alpha=0.6, c='purple', s=10)\n",
        "            plt.xlabel('Processing Time (seconds)')\n",
        "            plt.ylabel('Tokens Used')\n",
        "            plt.title('Processing Time vs Token Usage', fontsize=14, fontweight='bold')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            corr = self.df['processing_time'].corr(self.df['tokens_used'])\n",
        "            plt.text(0.05, 0.95, f'Correlation: {corr:.3f}',\n",
        "                    transform=plt.gca().transAxes,\n",
        "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        # 10. Reasoning Component Completeness\n",
        "        reasoning_components = ['sender_analysis', 'language_patterns', 'social_engineering',\n",
        "                              'technical_indicators', 'risk_assessment']\n",
        "        available_components = [col for col in reasoning_components if col in self.df.columns]\n",
        "\n",
        "        if available_components:\n",
        "            plt.subplot(4, 3, 10)\n",
        "            completeness = []\n",
        "            for component in available_components:\n",
        "                non_empty = (self.df[component].fillna('').astype(str).str.len() > 10).sum()\n",
        "                completeness.append(non_empty / len(self.df) * 100)\n",
        "\n",
        "            bars = plt.bar(range(len(available_components)), completeness,\n",
        "                          color='lightgreen', alpha=0.8)\n",
        "            plt.xlabel('Reasoning Components')\n",
        "            plt.ylabel('Completeness (%)')\n",
        "            plt.title('Reasoning Component Completeness', fontsize=14, fontweight='bold')\n",
        "            plt.xticks(range(len(available_components)),\n",
        "                      [comp.replace('_', '\\n') for comp in available_components], rotation=45)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            for bar, value in zip(bars, completeness):\n",
        "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                        f'{value:.1f}%', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        # 11. Word Clouds\n",
        "        if 'body' in self.df.columns and 'original_label' in self.df.columns:\n",
        "            # Legitimate emails word cloud\n",
        "            plt.subplot(4, 3, 11)\n",
        "            legitimate_texts = ' '.join(\n",
        "                self.df[self.df['original_label'] == 0]['body'].fillna('').astype(str)\n",
        "            )\n",
        "            if legitimate_texts.strip():\n",
        "                wordcloud = WordCloud(width=400, height=300, background_color='white',\n",
        "                                    stopwords=stop_words, max_words=50,\n",
        "                                    colormap='Greens').generate(legitimate_texts)\n",
        "                plt.imshow(wordcloud, interpolation='bilinear')\n",
        "                plt.title('Legitimate Emails - Top Words', fontsize=14, fontweight='bold')\n",
        "                plt.axis('off')\n",
        "\n",
        "        # 12. Phishing word cloud\n",
        "        if 'body' in self.df.columns and 'original_label' in self.df.columns:\n",
        "            plt.subplot(4, 3, 12)\n",
        "            phishing_texts = ' '.join(\n",
        "                self.df[self.df['original_label'] == 1]['body'].fillna('').astype(str)\n",
        "            )\n",
        "            if phishing_texts.strip():\n",
        "                wordcloud = WordCloud(width=400, height=300, background_color='white',\n",
        "                                    stopwords=stop_words, max_words=50,\n",
        "                                    colormap='Reds').generate(phishing_texts)\n",
        "                plt.imshow(wordcloud, interpolation='bilinear')\n",
        "                plt.title('Phishing Emails - Top Words', fontsize=14, fontweight='bold')\n",
        "                plt.axis('off')\n",
        "\n",
        "        plt.tight_layout(pad=3.0)\n",
        "        plt.savefig('o3_mini_complete_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"✅ Comprehensive visualization saved as 'o3_mini_complete_analysis.png'\")\n",
        "\n",
        "    def generate_summary_report(self):\n",
        "        \"\"\"Generate a comprehensive summary report\"\"\"\n",
        "        print(\"\\n📄 GENERATING SUMMARY REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Create detailed report\n",
        "        report = f\"\"\"\n",
        "# O3-MINI PHISHING DETECTION DATASET ANALYSIS REPORT\n",
        "\n",
        "## Research Project\n",
        "**Title:** Enhancing GPT-4o's Phishing Detection with o3-mini–Generated Chain-of-Thought Augmentation\n",
        "**Dataset:** o3_mini_optimized_dataset_20250709_185906.csv\n",
        "**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Dataset Overview\n",
        "- **Total Emails:** {len(self.df):,}\n",
        "- **Total Columns:** {len(self.df.columns)}\n",
        "- **Memory Usage:** {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n",
        "- **File Size:** {os.path.getsize(self.dataset_path) / 1024**2:.2f} MB\n",
        "\n",
        "## Key Findings\n",
        "\"\"\"\n",
        "\n",
        "        # Add key metrics\n",
        "        if 'prediction_correct' in self.df.columns:\n",
        "            accuracy = self.df['prediction_correct'].mean() * 100\n",
        "            report += f\"- **O3-Mini Model Accuracy:** {accuracy:.2f}%\\n\"\n",
        "\n",
        "        if 'original_label' in self.df.columns:\n",
        "            label_dist = self.df['original_label'].value_counts()\n",
        "            report += f\"- **Class Distribution:** Legitimate: {label_dist.get(0, 0):,}, Phishing: {label_dist.get(1, 0):,}\\n\"\n",
        "\n",
        "        if 'tokens_used' in self.df.columns:\n",
        "            avg_tokens = self.df['tokens_used'].mean()\n",
        "            total_tokens = self.df['tokens_used'].sum()\n",
        "            estimated_cost = (total_tokens / 1_000_000) * 1.10\n",
        "            report += f\"- **Token Usage:** {avg_tokens:.0f} avg per email, {total_tokens:,} total\\n\"\n",
        "            report += f\"- **Estimated Cost:** ${estimated_cost:.2f} total, ${estimated_cost/len(self.df):.4f} per email\\n\"\n",
        "\n",
        "        if 'processing_time' in self.df.columns:\n",
        "            avg_time = self.df['processing_time'].mean()\n",
        "            total_time = self.df['processing_time'].sum()\n",
        "            report += f\"- **Processing Time:** {avg_time:.2f}s avg per email, {total_time/3600:.2f}h total\\n\"\n",
        "\n",
        "        # Add research implications\n",
        "        report += f\"\"\"\n",
        "## Research Implications\n",
        "1. **Perfect Accuracy Achievement:** The o3-mini model achieved exceptional performance on this dataset\n",
        "2. **Comprehensive Reasoning:** All emails have complete 5-component reasoning chains\n",
        "3. **Scalable Processing:** Average processing time of {self.df['processing_time'].mean():.2f}s per email\n",
        "4. **Cost-Effective:** Estimated cost of ${(self.df['tokens_used'].sum() / 1_000_000) * 1.10 / len(self.df):.4f} per email analysis\n",
        "\n",
        "## Technical Quality\n",
        "- **Data Completeness:** High quality dataset with minimal missing values\n",
        "- **Reasoning Components:** All 5 reasoning dimensions consistently populated\n",
        "- **Chain-of-Thought Quality:** Comprehensive reasoning chains averaging {self.df['reasoning_chain'].str.len().mean():.0f} characters\n",
        "\n",
        "## Next Steps\n",
        "1. Use this dataset for GPT-4o fine-tuning\n",
        "2. Evaluate enhanced model against baselines\n",
        "3. Test on novel phishing attack vectors\n",
        "4. Publish methodology and results\n",
        "\n",
        "---\n",
        "Generated by O3-Mini Dataset Analyzer\n",
        "\"\"\"\n",
        "\n",
        "        # Save report\n",
        "        with open('o3_mini_analysis_report.md', 'w', encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Save analysis results as JSON\n",
        "        with open('o3_mini_analysis_results.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.analysis_results, f, indent=2, default=str)\n",
        "\n",
        "        print(\"✅ Summary report saved as 'o3_mini_analysis_report.md'\")\n",
        "        print(\"✅ Analysis results saved as 'o3_mini_analysis_results.json'\")\n",
        "\n",
        "        return report\n",
        "\n",
        "    def run_complete_analysis(self):\n",
        "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
        "        print(\"🚀 STARTING COMPLETE O3-MINI DATASET ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Load dataset\n",
        "        if not self.load_dataset():\n",
        "            return False\n",
        "\n",
        "        # Run all analyses\n",
        "        self.basic_overview()\n",
        "        self.analyze_text_characteristics()\n",
        "        self.analyze_model_performance()\n",
        "        self.analyze_processing_efficiency()\n",
        "        self.create_visualizations()\n",
        "        self.generate_summary_report()\n",
        "\n",
        "        print(f\"\\n🎉 ANALYSIS COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"📁 Generated Files:\")\n",
        "        print(\"   - o3_mini_complete_analysis.png (Comprehensive visualizations)\")\n",
        "        print(\"   - o3_mini_analysis_report.md (Detailed summary report)\")\n",
        "        print(\"   - o3_mini_analysis_results.json (Raw analysis data)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return True\n",
        "\n",
        "# ===========================\n",
        "# STEP 5: QUICK ANALYSIS FUNCTIONS\n",
        "# ===========================\n",
        "\n",
        "def quick_load_and_preview():\n",
        "    \"\"\"Quick function to load and preview the dataset\"\"\"\n",
        "    print(\"🔍 QUICK DATASET PREVIEW\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n",
        "        print(f\"✅ Dataset loaded successfully!\")\n",
        "        print(f\"📊 Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "\n",
        "        # Show first few rows\n",
        "        print(f\"\\n📋 First 3 rows:\")\n",
        "        print(df.head(3).to_string())\n",
        "\n",
        "        # Show column info\n",
        "        print(f\"\\n📈 Quick Statistics:\")\n",
        "        if 'original_label' in df.columns:\n",
        "            print(f\"   Class distribution: {df['original_label'].value_counts().to_dict()}\")\n",
        "        if 'prediction_correct' in df.columns:\n",
        "            print(f\"   Model accuracy: {df['prediction_correct'].mean()*100:.2f}%\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_quick_analysis():\n",
        "    \"\"\"Run a quick analysis - perfect for immediate results\"\"\"\n",
        "    print(\"⚡ QUICK ANALYSIS MODE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    df = quick_load_and_preview()\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    # Quick visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Class distribution\n",
        "    if 'original_label' in df.columns:\n",
        "        label_counts = df['original_label'].value_counts()\n",
        "        axes[0, 0].pie(label_counts.values, labels=['Legitimate', 'Phishing'],\n",
        "                      autopct='%1.1f%%', startangle=90)\n",
        "        axes[0, 0].set_title('Class Distribution')\n",
        "\n",
        "    # Model accuracy\n",
        "    if 'prediction_correct' in df.columns:\n",
        "        correct_counts = df['prediction_correct'].value_counts()\n",
        "        axes[0, 1].pie(correct_counts.values, labels=['Incorrect', 'Correct'],\n",
        "                      autopct='%1.1f%%', startangle=90)\n",
        "        axes[0, 1].set_title('Model Accuracy')\n",
        "\n",
        "    # Processing time\n",
        "    if 'processing_time' in df.columns:\n",
        "        axes[1, 0].hist(df['processing_time'], bins=30, alpha=0.7, color='skyblue')\n",
        "        axes[1, 0].set_title('Processing Time Distribution')\n",
        "        axes[1, 0].set_xlabel('Seconds')\n",
        "\n",
        "    # Token usage\n",
        "    if 'tokens_used' in df.columns:\n",
        "        axes[1, 1].hist(df['tokens_used'], bins=30, alpha=0.7, color='lightcoral')\n",
        "        axes[1, 1].set_title('Token Usage Distribution')\n",
        "        axes[1, 1].set_xlabel('Tokens')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✅ Quick analysis complete!\")\n",
        "    return df\n",
        "\n",
        "# ===========================\n",
        "# STEP 6: MAIN EXECUTION\n",
        "# ===========================\n",
        "\n",
        "def run_complete_analysis():\n",
        "    \"\"\"Main function to run the complete analysis\"\"\"\n",
        "    analyzer = O3MiniDatasetAnalyzer(DATASET_PATH)\n",
        "    return analyzer.run_complete_analysis()\n",
        "\n",
        "# ===========================\n",
        "# STEP 7: READY-TO-RUN COMMANDS\n",
        "# ===========================\n",
        "\n",
        "print(\"🔬 O3-MINI DATASET ANALYZER - READY TO RUN\")\n",
        "print(\"=\"*80)\n",
        "print(\"📂 Dataset Path:\", DATASET_PATH)\n",
        "print(\"\\nChoose your analysis level:\")\n",
        "print(\"1. Quick Preview: run quick_load_and_preview()\")\n",
        "print(\"2. Quick Analysis: run_quick_analysis()\")\n",
        "print(\"3. Complete Analysis: run_complete_analysis()\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Uncomment the line below to run complete analysis immediately\n",
        "run_complete_analysis()\n",
        "\n",
        "# Or run quick analysis first\n",
        "# df = run_quick_analysis()"
      ],
      "metadata": {
        "id": "VfVJGFkhyBr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete GPT-4o Mini Fine-tuning Pipeline\n",
        "Converts o3-mini generated dataset to GPT-4o mini fine-tuning format and handles training\n",
        "\n",
        "Research: Enhancing GPT-4o's Phishing Detection with o3-mini–Generated Chain-of-Thought Augmentation\n",
        "\n",
        "FIXED VERSION - Key Improvements:\n",
        "- Corrected token limits (128K context window)\n",
        "- Enhanced data validation\n",
        "- Improved system prompt\n",
        "- Complete fine-tuning workflow\n",
        "- Removed cost calculations (tokens only)\n",
        "- Added monitoring capabilities\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages\n",
        "!pip install tiktoken openai -q\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"🔧 COMPLETE GPT-4o MINI FINE-TUNING PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Dataset configuration - FIXED VALUES\n",
        "DATASET_PATH = \"/content/drive/MyDrive/phishing_detection_final/output/final_datasets/o3_mini_optimized_dataset_20250709_185906.csv\"\n",
        "\n",
        "# GPT-4o mini parameters - CORRECTED\n",
        "MAX_TOKENS_PER_EXAMPLE = 100000    # Conservative limit within 128K context\n",
        "ENCODING_NAME = \"o200k_base\"       # GPT-4o family encoding\n",
        "TARGET_TRAINING_EXAMPLES = 5000    # Recommended range\n",
        "TARGET_VALIDATION_EXAMPLES = 1000\n",
        "\n",
        "# OpenAI API configuration\n",
        "OPENAI_API_KEY = \"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\"\n",
        "\n",
        "print(f\"📂 Source Dataset: {DATASET_PATH}\")\n",
        "print(f\"🎯 Target Training Examples: {TARGET_TRAINING_EXAMPLES:,}\")\n",
        "print(f\"🎯 Target Validation Examples: {TARGET_VALIDATION_EXAMPLES:,}\")\n",
        "print(f\"🪙 Token Limit per Example: {MAX_TOKENS_PER_EXAMPLE:,}\")\n",
        "\n",
        "class CompleteFinetuningPipeline:\n",
        "    \"\"\"Complete pipeline for GPT-4o mini fine-tuning with monitoring\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path, api_key):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.df = None\n",
        "        self.encoding = tiktoken.get_encoding(ENCODING_NAME)\n",
        "        self.training_data = []\n",
        "        self.validation_data = []\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "        # Fine-tuning tracking\n",
        "        self.training_file_id = None\n",
        "        self.validation_file_id = None\n",
        "        self.job_id = None\n",
        "        self.fine_tuned_model = None\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"Load and examine the dataset\"\"\"\n",
        "        print(\"\\n📊 LOADING DATASET\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            self.df = pd.read_csv(self.dataset_path, encoding='utf-8')\n",
        "            print(f\"✅ Dataset loaded: {len(self.df):,} rows\")\n",
        "\n",
        "            # Show relevant columns\n",
        "            key_columns = ['subject', 'body', 'original_label', 'original_label_name',\n",
        "                          'reasoning_chain', 'sender_analysis', 'language_patterns',\n",
        "                          'social_engineering', 'technical_indicators', 'risk_assessment']\n",
        "\n",
        "            print(f\"\\n📋 KEY COLUMNS FOR FINE-TUNING:\")\n",
        "            for i, col in enumerate(key_columns, 1):\n",
        "                if col in self.df.columns:\n",
        "                    non_null = self.df[col].notna().sum()\n",
        "                    print(f\"  {i:2d}. {col:<25}: {non_null:,}/{len(self.df):,} ({non_null/len(self.df)*100:.1f}%)\")\n",
        "                else:\n",
        "                    print(f\"  {i:2d}. {col:<25}: ❌ Missing\")\n",
        "\n",
        "            # Class distribution\n",
        "            if 'original_label' in self.df.columns:\n",
        "                dist = self.df['original_label'].value_counts()\n",
        "                print(f\"\\n🎯 Class Distribution:\")\n",
        "                print(f\"   Legitimate (0): {dist.get(0, 0):,}\")\n",
        "                print(f\"   Phishing (1):   {dist.get(1, 0):,}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading dataset: {e}\")\n",
        "            return False\n",
        "\n",
        "    def create_enhanced_system_prompt(self):\n",
        "        \"\"\"Enhanced system prompt for phishing detection\"\"\"\n",
        "        return \"\"\"You are an expert cybersecurity analyst specializing in email threat detection. Your task is to analyze emails for phishing indicators using a structured approach.\n",
        "\n",
        "ANALYSIS FRAMEWORK:\n",
        "1. SENDER ANALYSIS: Examine sender authenticity, domain reputation, email authentication indicators\n",
        "2. LANGUAGE PATTERNS: Analyze grammar, writing style, urgency indicators, linguistic manipulation\n",
        "3. SOCIAL ENGINEERING: Identify psychological manipulation, emotional triggers, deceptive tactics\n",
        "4. TECHNICAL INDICATORS: Assess URLs, attachments, technical attack vectors, suspicious elements\n",
        "5. RISK ASSESSMENT: Synthesize findings and provide evidence-based classification\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "- Provide detailed analysis for each component\n",
        "- End with \"**FINAL CLASSIFICATION:** LEGITIMATE\" or \"**FINAL CLASSIFICATION:** PHISHING\"\n",
        "- Be specific about evidence and reasoning\n",
        "\n",
        "Your analysis should be thorough, evidence-based, and follow cybersecurity best practices.\"\"\"\n",
        "\n",
        "    def enhanced_data_validation(self, row):\n",
        "        \"\"\"Enhanced validation for training examples\"\"\"\n",
        "        subject = str(row.get('subject', '')).strip()\n",
        "        body = str(row.get('body', '')).strip()\n",
        "\n",
        "        # Check minimum content length\n",
        "        if len(subject) < 5 or len(body) < 50:\n",
        "            return False, \"Content too short\"\n",
        "\n",
        "        # Check for actual reasoning content\n",
        "        reasoning_fields = ['sender_analysis', 'language_patterns', 'social_engineering',\n",
        "                           'technical_indicators', 'risk_assessment']\n",
        "\n",
        "        valid_reasoning = 0\n",
        "        for field in reasoning_fields:\n",
        "            content = str(row.get(field, '')).strip()\n",
        "            if content and content != 'nan' and len(content) > 20:\n",
        "                valid_reasoning += 1\n",
        "\n",
        "        if valid_reasoning < 3:  # Require at least 3 reasoning components\n",
        "            return False, \"Insufficient reasoning content\"\n",
        "\n",
        "        return True, \"Valid\"\n",
        "\n",
        "    def create_training_example(self, row):\n",
        "        \"\"\"Create a single training example in GPT-4o mini format\"\"\"\n",
        "\n",
        "        # Validate first\n",
        "        is_valid, reason = self.enhanced_data_validation(row)\n",
        "        if not is_valid:\n",
        "            raise ValueError(f\"Invalid data: {reason}\")\n",
        "\n",
        "        # Extract data\n",
        "        subject = str(row.get('subject', 'No Subject')).strip()\n",
        "        body = str(row.get('body', '')).strip()\n",
        "        label = row.get('original_label', 0)\n",
        "        label_name = \"LEGITIMATE\" if label == 0 else \"PHISHING\"\n",
        "\n",
        "        # Get reasoning components\n",
        "        reasoning_components = {\n",
        "            'sender_analysis': str(row.get('sender_analysis', '')).strip(),\n",
        "            'language_patterns': str(row.get('language_patterns', '')).strip(),\n",
        "            'social_engineering': str(row.get('social_engineering', '')).strip(),\n",
        "            'technical_indicators': str(row.get('technical_indicators', '')).strip(),\n",
        "            'risk_assessment': str(row.get('risk_assessment', '')).strip()\n",
        "        }\n",
        "\n",
        "        # Create user message (email content)\n",
        "        user_content = f\"\"\"Please analyze this email for phishing indicators:\n",
        "\n",
        "SUBJECT: {subject}\n",
        "\n",
        "BODY: {body}\n",
        "\n",
        "Provide detailed analysis through all five components and your final classification.\"\"\"\n",
        "\n",
        "        # Create assistant response (detailed reasoning + classification)\n",
        "        assistant_parts = []\n",
        "\n",
        "        for component, content in reasoning_components.items():\n",
        "            if content and content != 'nan' and len(content) > 10:\n",
        "                component_title = component.replace('_', ' ').title()\n",
        "                assistant_parts.append(f\"**{component_title.upper()}:**\\n{content}\")\n",
        "\n",
        "        # Add final classification\n",
        "        assistant_parts.append(f\"**FINAL CLASSIFICATION:** {label_name}\")\n",
        "\n",
        "        assistant_content = \"\\n\\n\".join(assistant_parts)\n",
        "\n",
        "        # Create the training example\n",
        "        example = {\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": self.create_enhanced_system_prompt()\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": user_content\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": assistant_content\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        return example\n",
        "\n",
        "    def count_tokens(self, messages):\n",
        "        \"\"\"Count tokens in a message sequence\"\"\"\n",
        "        total_tokens = 0\n",
        "\n",
        "        for message in messages:\n",
        "            # Count tokens for role and content\n",
        "            total_tokens += len(self.encoding.encode(message[\"role\"]))\n",
        "            total_tokens += len(self.encoding.encode(message[\"content\"]))\n",
        "            total_tokens += 3  # OpenAI message formatting tokens\n",
        "\n",
        "        total_tokens += 3  # Additional formatting tokens\n",
        "        return total_tokens\n",
        "\n",
        "    def prepare_training_data(self):\n",
        "        \"\"\"Prepare training and validation datasets with enhanced validation\"\"\"\n",
        "        print(f\"\\n🔧 PREPARING TRAINING DATA\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Filter out rows with missing critical data\n",
        "        required_cols = ['subject', 'body', 'original_label']\n",
        "        mask = self.df[required_cols].notna().all(axis=1)\n",
        "        clean_df = self.df[mask].copy()\n",
        "\n",
        "        print(f\"📊 Clean examples: {len(clean_df):,}/{len(self.df):,}\")\n",
        "\n",
        "        # Stratified sampling to maintain class balance\n",
        "        legitimate_df = clean_df[clean_df['original_label'] == 0]\n",
        "        phishing_df = clean_df[clean_df['original_label'] == 1]\n",
        "\n",
        "        print(f\"   Legitimate: {len(legitimate_df):,}\")\n",
        "        print(f\"   Phishing:   {len(phishing_df):,}\")\n",
        "\n",
        "        # Calculate samples per class\n",
        "        total_needed = TARGET_TRAINING_EXAMPLES + TARGET_VALIDATION_EXAMPLES\n",
        "        samples_per_class = min(\n",
        "            total_needed // 2,\n",
        "            min(len(legitimate_df), len(phishing_df))\n",
        "        )\n",
        "\n",
        "        print(f\"📝 Sampling {samples_per_class:,} examples per class\")\n",
        "\n",
        "        # Sample data\n",
        "        legit_sample = legitimate_df.sample(n=samples_per_class, random_state=42)\n",
        "        phish_sample = phishing_df.sample(n=samples_per_class, random_state=42)\n",
        "\n",
        "        # Combine and shuffle\n",
        "        combined_sample = pd.concat([legit_sample, phish_sample]).sample(frac=1, random_state=42)\n",
        "\n",
        "        print(f\"✅ Total examples: {len(combined_sample):,}\")\n",
        "\n",
        "        # Process examples and check token limits\n",
        "        valid_examples = []\n",
        "        token_counts = []\n",
        "        skipped_long = 0\n",
        "        skipped_invalid = 0\n",
        "\n",
        "        print(f\"\\n🪙 PROCESSING EXAMPLES AND COUNTING TOKENS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for idx, row in combined_sample.iterrows():\n",
        "            try:\n",
        "                example = self.create_training_example(row)\n",
        "                token_count = self.count_tokens(example[\"messages\"])\n",
        "\n",
        "                if token_count <= MAX_TOKENS_PER_EXAMPLE:\n",
        "                    valid_examples.append(example)\n",
        "                    token_counts.append(token_count)\n",
        "                else:\n",
        "                    skipped_long += 1\n",
        "\n",
        "            except ValueError as e:\n",
        "                skipped_invalid += 1\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Error processing row {idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ Valid examples: {len(valid_examples):,}\")\n",
        "        print(f\"❌ Skipped (too long): {skipped_long:,}\")\n",
        "        print(f\"❌ Skipped (invalid): {skipped_invalid:,}\")\n",
        "\n",
        "        # Token statistics\n",
        "        if token_counts:\n",
        "            print(f\"\\n📊 TOKEN STATISTICS:\")\n",
        "            print(f\"   Mean: {np.mean(token_counts):.0f}\")\n",
        "            print(f\"   Median: {np.median(token_counts):.0f}\")\n",
        "            print(f\"   Min/Max: {min(token_counts):,} / {max(token_counts):,}\")\n",
        "            print(f\"   95th percentile: {np.percentile(token_counts, 95):.0f}\")\n",
        "\n",
        "            # Token count only (no cost)\n",
        "            total_tokens = sum(token_counts)\n",
        "            print(f\"\\n🪙 TOKEN SUMMARY:\")\n",
        "            print(f\"   Total training tokens: {total_tokens:,}\")\n",
        "\n",
        "        # Split into training and validation\n",
        "        train_size = min(TARGET_TRAINING_EXAMPLES, len(valid_examples) * 4 // 5)\n",
        "\n",
        "        self.training_data = valid_examples[:train_size]\n",
        "        self.validation_data = valid_examples[train_size:train_size + TARGET_VALIDATION_EXAMPLES]\n",
        "\n",
        "        print(f\"\\n📊 FINAL SPLIT:\")\n",
        "        print(f\"   Training examples: {len(self.training_data):,}\")\n",
        "        print(f\"   Validation examples: {len(self.validation_data):,}\")\n",
        "\n",
        "        return len(valid_examples) > 0\n",
        "\n",
        "    def validate_finetuning_requirements(self):\n",
        "        \"\"\"Validate data meets OpenAI fine-tuning requirements\"\"\"\n",
        "        print(f\"\\n🔍 VALIDATING FINE-TUNING REQUIREMENTS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        issues = []\n",
        "\n",
        "        # Check minimum examples\n",
        "        if len(self.training_data) < 10:\n",
        "            issues.append(\"Minimum 10 training examples required\")\n",
        "\n",
        "        # Check message structure\n",
        "        for i, example in enumerate(self.training_data[:5]):  # Check first 5\n",
        "            messages = example.get(\"messages\", [])\n",
        "\n",
        "            if len(messages) != 3:\n",
        "                issues.append(f\"Example {i}: Should have exactly 3 messages (system, user, assistant)\")\n",
        "\n",
        "            roles = [msg.get(\"role\") for msg in messages]\n",
        "            if roles != [\"system\", \"user\", \"assistant\"]:\n",
        "                issues.append(f\"Example {i}: Incorrect role sequence: {roles}\")\n",
        "\n",
        "            # Check assistant response ends with classification\n",
        "            assistant_content = messages[-1].get(\"content\", \"\")\n",
        "            if \"FINAL CLASSIFICATION:\" not in assistant_content:\n",
        "                issues.append(f\"Example {i}: Missing final classification\")\n",
        "\n",
        "        if issues:\n",
        "            print(\"❌ VALIDATION ISSUES:\")\n",
        "            for issue in issues:\n",
        "                print(f\"   - {issue}\")\n",
        "            return False\n",
        "        else:\n",
        "            print(\"✅ All validation checks passed\")\n",
        "            return True\n",
        "\n",
        "    def save_jsonl_files(self):\n",
        "        \"\"\"Save training and validation files in JSONL format\"\"\"\n",
        "        print(f\"\\n💾 SAVING JSONL FILES\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Save training file\n",
        "        training_file = f\"gpt4o_mini_training_{timestamp}.jsonl\"\n",
        "        with open(training_file, 'w', encoding='utf-8') as f:\n",
        "            for example in self.training_data:\n",
        "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"✅ Training file: {training_file}\")\n",
        "        print(f\"   Examples: {len(self.training_data):,}\")\n",
        "\n",
        "        # Save validation file\n",
        "        validation_file = f\"gpt4o_mini_validation_{timestamp}.jsonl\"\n",
        "        with open(validation_file, 'w', encoding='utf-8') as f:\n",
        "            for example in self.validation_data:\n",
        "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"✅ Validation file: {validation_file}\")\n",
        "        print(f\"   Examples: {len(self.validation_data):,}\")\n",
        "\n",
        "        # Create a sample preview file\n",
        "        sample_file = f\"gpt4o_mini_sample_{timestamp}.json\"\n",
        "        sample_data = {\n",
        "            \"sample_training_example\": self.training_data[0] if self.training_data else {},\n",
        "            \"dataset_info\": {\n",
        "                \"training_examples\": len(self.training_data),\n",
        "                \"validation_examples\": len(self.validation_data),\n",
        "                \"created\": timestamp,\n",
        "                \"purpose\": \"GPT-4o mini phishing detection fine-tuning\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(sample_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(sample_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"✅ Sample file: {sample_file}\")\n",
        "\n",
        "        return training_file, validation_file, sample_file\n",
        "\n",
        "    def upload_training_files(self, training_file, validation_file):\n",
        "        \"\"\"Upload training files to OpenAI\"\"\"\n",
        "        print(f\"\\n🚀 UPLOADING FILES TO OPENAI\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            # Upload training file\n",
        "            print(\"📤 Uploading training file...\")\n",
        "            with open(training_file, \"rb\") as f:\n",
        "                training_upload = self.client.files.create(\n",
        "                    file=f,\n",
        "                    purpose=\"fine-tune\"\n",
        "                )\n",
        "            self.training_file_id = training_upload.id\n",
        "            print(f\"✅ Training file uploaded: {self.training_file_id}\")\n",
        "\n",
        "            # Upload validation file\n",
        "            print(\"📤 Uploading validation file...\")\n",
        "            with open(validation_file, \"rb\") as f:\n",
        "                validation_upload = self.client.files.create(\n",
        "                    file=f,\n",
        "                    purpose=\"fine-tune\"\n",
        "                )\n",
        "            self.validation_file_id = validation_upload.id\n",
        "            print(f\"✅ Validation file uploaded: {self.validation_file_id}\")\n",
        "\n",
        "            # Verify uploads\n",
        "            print(f\"\\n📋 UPLOAD VERIFICATION:\")\n",
        "            train_info = self.client.files.retrieve(self.training_file_id)\n",
        "            val_info = self.client.files.retrieve(self.validation_file_id)\n",
        "\n",
        "            print(f\"   Training: {train_info.filename} ({train_info.bytes:,} bytes) - {train_info.status}\")\n",
        "            print(f\"   Validation: {val_info.filename} ({val_info.bytes:,} bytes) - {val_info.status}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Upload failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def create_finetuning_job(self):\n",
        "        \"\"\"Create fine-tuning job\"\"\"\n",
        "        print(f\"\\n🎯 CREATING FINE-TUNING JOB\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            # Create fine-tuning job\n",
        "            job = self.client.fine_tuning.jobs.create(\n",
        "                training_file=self.training_file_id,\n",
        "                validation_file=self.validation_file_id,\n",
        "                model=\"gpt-4o-mini-2024-07-18\",\n",
        "                suffix=\"phishing-detection\"\n",
        "            )\n",
        "\n",
        "            self.job_id = job.id\n",
        "\n",
        "            print(f\"✅ Fine-tuning job created!\")\n",
        "            print(f\"   Job ID: {self.job_id}\")\n",
        "            print(f\"   Model: {job.model}\")\n",
        "            print(f\"   Status: {job.status}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Job creation failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def monitor_training(self):\n",
        "        \"\"\"Monitor training progress\"\"\"\n",
        "        print(f\"\\n📊 MONITORING TRAINING PROGRESS\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Job ID: {self.job_id}\")\n",
        "        print(\"Status updates will appear below...\")\n",
        "        print(\"This may take 10-60 minutes depending on dataset size.\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            last_status = None\n",
        "\n",
        "            while True:\n",
        "                # Get job status\n",
        "                job = self.client.fine_tuning.jobs.retrieve(self.job_id)\n",
        "\n",
        "                if job.status != last_status:\n",
        "                    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "                    print(f\"[{timestamp}] Status: {job.status}\")\n",
        "\n",
        "                    if hasattr(job, 'trained_tokens') and job.trained_tokens:\n",
        "                        print(f\"[{timestamp}] Trained tokens: {job.trained_tokens:,}\")\n",
        "\n",
        "                    last_status = job.status\n",
        "\n",
        "                # Check if completed\n",
        "                if job.status == \"succeeded\":\n",
        "                    self.fine_tuned_model = job.fine_tuned_model\n",
        "                    print(f\"\\n🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "                    print(f\"   Fine-tuned model: {self.fine_tuned_model}\")\n",
        "\n",
        "                    # Show final metrics if available\n",
        "                    if hasattr(job, 'result_files') and job.result_files:\n",
        "                        print(f\"   Result files: {len(job.result_files)} available\")\n",
        "\n",
        "                    return True\n",
        "\n",
        "                elif job.status == \"failed\":\n",
        "                    print(f\"\\n❌ TRAINING FAILED\")\n",
        "                    if hasattr(job, 'error') and job.error:\n",
        "                        print(f\"   Error: {job.error}\")\n",
        "                    return False\n",
        "\n",
        "                elif job.status == \"cancelled\":\n",
        "                    print(f\"\\n⚠️ TRAINING CANCELLED\")\n",
        "                    return False\n",
        "\n",
        "                # Wait before next check\n",
        "                time.sleep(30)  # Check every 30 seconds\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(f\"\\n⚠️ Monitoring interrupted by user\")\n",
        "            print(f\"Job {self.job_id} is still running. Check status manually.\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Monitoring error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_job_status(self):\n",
        "        \"\"\"Get current job status\"\"\"\n",
        "        if not self.job_id:\n",
        "            print(\"❌ No job ID available\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            job = self.client.fine_tuning.jobs.retrieve(self.job_id)\n",
        "            return job\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error getting job status: {e}\")\n",
        "            return None\n",
        "\n",
        "    def test_fine_tuned_model(self, test_email_subject=\"Urgent Account Verification\",\n",
        "                            test_email_body=\"Your account will be suspended unless you verify immediately. Click here: http://suspicious-link.com\"):\n",
        "        \"\"\"Test the fine-tuned model\"\"\"\n",
        "        if not self.fine_tuned_model:\n",
        "            print(\"❌ No fine-tuned model available\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\n🧪 TESTING FINE-TUNED MODEL\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Model: {self.fine_tuned_model}\")\n",
        "\n",
        "        try:\n",
        "            test_messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": self.create_enhanced_system_prompt()\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"Please analyze this email for phishing indicators:\n",
        "\n",
        "SUBJECT: {test_email_subject}\n",
        "\n",
        "BODY: {test_email_body}\n",
        "\n",
        "Provide detailed analysis through all five components and your final classification.\"\"\"\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.fine_tuned_model,\n",
        "                messages=test_messages,\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            print(f\"✅ Test successful!\")\n",
        "            print(f\"\\n📧 Test Email:\")\n",
        "            print(f\"Subject: {test_email_subject}\")\n",
        "            print(f\"Body: {test_email_body}\")\n",
        "            print(f\"\\n🤖 Model Response:\")\n",
        "            print(response.choices[0].message.content)\n",
        "\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Test failed: {e}\")\n",
        "            return None\n",
        "\n",
        "def run_complete_pipeline():\n",
        "    \"\"\"Run the complete fine-tuning pipeline\"\"\"\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipeline = CompleteFinetuningPipeline(DATASET_PATH, OPENAI_API_KEY)\n",
        "\n",
        "    # Step 1: Load dataset\n",
        "    if not pipeline.load_dataset():\n",
        "        return False\n",
        "\n",
        "    # Step 2: Prepare training data\n",
        "    if not pipeline.prepare_training_data():\n",
        "        print(\"❌ Failed to prepare training data\")\n",
        "        return False\n",
        "\n",
        "    # Step 3: Validate requirements\n",
        "    if not pipeline.validate_finetuning_requirements():\n",
        "        print(\"❌ Validation failed\")\n",
        "        return False\n",
        "\n",
        "    # Step 4: Save JSONL files\n",
        "    training_file, validation_file, sample_file = pipeline.save_jsonl_files()\n",
        "\n",
        "    # Step 5: Upload files to OpenAI\n",
        "    if not pipeline.upload_training_files(training_file, validation_file):\n",
        "        print(\"❌ File upload failed\")\n",
        "        return False\n",
        "\n",
        "    # Step 6: Create fine-tuning job\n",
        "    if not pipeline.create_finetuning_job():\n",
        "        print(\"❌ Job creation failed\")\n",
        "        return False\n",
        "\n",
        "    # Step 7: Monitor training\n",
        "    training_success = pipeline.monitor_training()\n",
        "\n",
        "    if training_success:\n",
        "        # Step 8: Test the model\n",
        "        pipeline.test_fine_tuned_model()\n",
        "\n",
        "        print(f\"\\n🎉 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"📁 FILES CREATED:\")\n",
        "        print(f\"   🚀 Training: {training_file}\")\n",
        "        print(f\"   🔍 Validation: {validation_file}\")\n",
        "        print(f\"   📋 Sample: {sample_file}\")\n",
        "        print(f\"\\n🤖 FINE-TUNED MODEL:\")\n",
        "        print(f\"   Model ID: {pipeline.fine_tuned_model}\")\n",
        "        print(f\"   Job ID: {pipeline.job_id}\")\n",
        "        print(f\"\\n✅ Ready for production use!\")\n",
        "\n",
        "    return training_success\n",
        "\n",
        "# Manual control functions for step-by-step execution\n",
        "def run_data_preparation_only():\n",
        "    \"\"\"Run only data preparation steps\"\"\"\n",
        "    pipeline = CompleteFinetuningPipeline(DATASET_PATH, OPENAI_API_KEY)\n",
        "\n",
        "    if not pipeline.load_dataset():\n",
        "        return None\n",
        "\n",
        "    if not pipeline.prepare_training_data():\n",
        "        return None\n",
        "\n",
        "    if not pipeline.validate_finetuning_requirements():\n",
        "        return None\n",
        "\n",
        "    training_file, validation_file, sample_file = pipeline.save_jsonl_files()\n",
        "\n",
        "    print(f\"\\n✅ DATA PREPARATION COMPLETE\")\n",
        "    print(f\"Files ready for fine-tuning: {training_file}, {validation_file}\")\n",
        "\n",
        "    return pipeline, training_file, validation_file\n",
        "\n",
        "def manual_finetuning_workflow(training_file, validation_file):\n",
        "    \"\"\"Manual fine-tuning workflow for step-by-step control\"\"\"\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    print(f\"\\n🚀 MANUAL FINE-TUNING WORKFLOW\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Upload files\n",
        "    print(\"1. Uploading training files...\")\n",
        "    with open(training_file, \"rb\") as f:\n",
        "        train_upload = client.files.create(file=f, purpose=\"fine-tune\")\n",
        "\n",
        "    with open(validation_file, \"rb\") as f:\n",
        "        val_upload = client.files.create(file=f, purpose=\"fine-tune\")\n",
        "\n",
        "    print(f\"   Training file ID: {train_upload.id}\")\n",
        "    print(f\"   Validation file ID: {val_upload.id}\")\n",
        "\n",
        "    # Create job\n",
        "    print(\"2. Creating fine-tuning job...\")\n",
        "    job = client.fine_tuning.jobs.create(\n",
        "        training_file=train_upload.id,\n",
        "        validation_file=val_upload.id,\n",
        "        model=\"gpt-4o-mini-2024-07-18\",\n",
        "        suffix=\"phishing-detection\"\n",
        "    )\n",
        "\n",
        "    print(f\"   Job ID: {job.id}\")\n",
        "    print(f\"   Status: {job.status}\")\n",
        "\n",
        "    return client, job.id\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"GPT-4o Mini Fine-tuning Pipeline\")\n",
        "    print(\"Choose execution mode:\")\n",
        "    print(\"1. run_complete_pipeline() - Full automated pipeline\")\n",
        "    print(\"2. run_data_preparation_only() - Data prep only\")\n",
        "    print(\"3. manual_finetuning_workflow() - Step by step control\")\n",
        "\n",
        "# Run the complete pipeline\n",
        "run_complete_pipeline()"
      ],
      "metadata": {
        "id": "WoFLr4N4zDdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Standalone GPT-4o Mini Fine-tuning: Upload, Initiate, Monitor\n",
        "Simplified script for file upload, job creation, and monitoring\n",
        "\n",
        "Usage:\n",
        "1. Set your API key\n",
        "2. Update file paths\n",
        "3. Run the functions step by step\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION - UPDATE THESE VALUES\n",
        "# ==========================================\n",
        "\n",
        "# Your OpenAI API Key - GET A NEW ONE FROM: https://platform.openai.com/api-keys\n",
        "API_KEY = \"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\"  # ⚠️ REPLACE WITH YOUR ACTUAL API KEY\n",
        "\n",
        "# File paths (update these to your actual file locations)\n",
        "TRAINING_FILE_PATH = \"/content/gpt4o_mini_training_20250716_065744.jsonl\"\n",
        "VALIDATION_FILE_PATH = \"/content/gpt4o_mini_validation_20250716_065744.jsonl\"\n",
        "\n",
        "# Fine-tuning configuration\n",
        "MODEL_NAME = \"gpt-4o-mini-2024-07-18\"\n",
        "SUFFIX = \"phishing-detection\"\n",
        "\n",
        "print(\"🚀 STANDALONE GPT-4o MINI FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ==========================================\n",
        "# OPENAI CLIENT SETUP\n",
        "# ==========================================\n",
        "\n",
        "def setup_client():\n",
        "    \"\"\"Initialize OpenAI client with API key validation\"\"\"\n",
        "    global client\n",
        "\n",
        "    if API_KEY == \"your-api-key-here\":\n",
        "        print(\"❌ ERROR: Please update the API_KEY variable with your actual OpenAI API key\")\n",
        "        print(\"   Get your API key from: https://platform.openai.com/api-keys\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "        # Test API key by listing models\n",
        "        models = client.models.list()\n",
        "        print(\"✅ API key validated successfully\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ API key validation failed: {e}\")\n",
        "        print(\"   Please check your API key at: https://platform.openai.com/api-keys\")\n",
        "        return False\n",
        "\n",
        "# ==========================================\n",
        "# STEP 1: UPLOAD FILES\n",
        "# ==========================================\n",
        "\n",
        "def upload_files():\n",
        "    \"\"\"Upload training and validation files to OpenAI\"\"\"\n",
        "    global training_file_id, validation_file_id\n",
        "\n",
        "    print(f\"\\n📤 STEP 1: UPLOADING FILES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Upload training file\n",
        "        print(f\"📁 Uploading training file: {TRAINING_FILE_PATH}\")\n",
        "        with open(TRAINING_FILE_PATH, \"rb\") as f:\n",
        "            training_upload = client.files.create(\n",
        "                file=f,\n",
        "                purpose=\"fine-tune\"\n",
        "            )\n",
        "        training_file_id = training_upload.id\n",
        "        print(f\"✅ Training file uploaded successfully\")\n",
        "        print(f\"   File ID: {training_file_id}\")\n",
        "        print(f\"   Size: {training_upload.bytes:,} bytes\")\n",
        "\n",
        "        # Upload validation file\n",
        "        print(f\"\\n📁 Uploading validation file: {VALIDATION_FILE_PATH}\")\n",
        "        with open(VALIDATION_FILE_PATH, \"rb\") as f:\n",
        "            validation_upload = client.files.create(\n",
        "                file=f,\n",
        "                purpose=\"fine-tune\"\n",
        "            )\n",
        "        validation_file_id = validation_upload.id\n",
        "        print(f\"✅ Validation file uploaded successfully\")\n",
        "        print(f\"   File ID: {validation_file_id}\")\n",
        "        print(f\"   Size: {validation_upload.bytes:,} bytes\")\n",
        "\n",
        "        # Verify files are processed\n",
        "        print(f\"\\n🔍 Verifying file processing...\")\n",
        "\n",
        "        # Check training file status\n",
        "        train_info = client.files.retrieve(training_file_id)\n",
        "        print(f\"   Training file: {train_info.status}\")\n",
        "\n",
        "        # Check validation file status\n",
        "        val_info = client.files.retrieve(validation_file_id)\n",
        "        print(f\"   Validation file: {val_info.status}\")\n",
        "\n",
        "        if train_info.status == \"processed\" and val_info.status == \"processed\":\n",
        "            print(\"✅ Both files processed and ready for fine-tuning\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"⚠️  Files still processing. Wait a few seconds and check status.\")\n",
        "            return True  # Can still proceed\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"❌ File not found: {e}\")\n",
        "        print(\"   Please check the file paths and make sure files exist\")\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Upload failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# ==========================================\n",
        "# STEP 2: CREATE FINE-TUNING JOB\n",
        "# ==========================================\n",
        "\n",
        "def create_finetuning_job():\n",
        "    \"\"\"Create fine-tuning job\"\"\"\n",
        "    global job_id\n",
        "\n",
        "    print(f\"\\n🎯 STEP 2: CREATING FINE-TUNING JOB\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Create fine-tuning job\n",
        "        job = client.fine_tuning.jobs.create(\n",
        "            training_file=training_file_id,\n",
        "            validation_file=validation_file_id,\n",
        "            model=MODEL_NAME,\n",
        "            suffix=SUFFIX\n",
        "        )\n",
        "\n",
        "        job_id = job.id\n",
        "\n",
        "        print(f\"✅ Fine-tuning job created successfully!\")\n",
        "        print(f\"   Job ID: {job_id}\")\n",
        "        print(f\"   Model: {job.model}\")\n",
        "        print(f\"   Status: {job.status}\")\n",
        "        print(f\"   Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Job creation failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# ==========================================\n",
        "# STEP 3: MONITOR TRAINING\n",
        "# ==========================================\n",
        "\n",
        "def monitor_training():\n",
        "    \"\"\"Monitor training progress with real-time updates\"\"\"\n",
        "    print(f\"\\n📊 STEP 3: MONITORING TRAINING PROGRESS\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Job ID: {job_id}\")\n",
        "    print(\"⏱️  Monitoring started. Training typically takes 10-60 minutes.\")\n",
        "    print(\"   Press Ctrl+C to stop monitoring (job will continue running)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        last_status = None\n",
        "        start_time = time.time()\n",
        "\n",
        "        while True:\n",
        "            # Get job status\n",
        "            job = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "            # Show status updates\n",
        "            if job.status != last_status:\n",
        "                elapsed = time.time() - start_time\n",
        "                timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "\n",
        "                print(f\"[{timestamp}] Status: {job.status.upper()} (Elapsed: {elapsed/60:.1f}m)\")\n",
        "\n",
        "                # Show additional info if available\n",
        "                if hasattr(job, 'trained_tokens') and job.trained_tokens:\n",
        "                    print(f\"[{timestamp}] Trained tokens: {job.trained_tokens:,}\")\n",
        "\n",
        "                if hasattr(job, 'training_file') and job.training_file:\n",
        "                    print(f\"[{timestamp}] Training file: {job.training_file}\")\n",
        "\n",
        "                last_status = job.status\n",
        "\n",
        "            # Check completion status\n",
        "            if job.status == \"succeeded\":\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"\\n🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "                print(f\"   Total time: {elapsed/60:.1f} minutes\")\n",
        "                print(f\"   Fine-tuned model: {job.fine_tuned_model}\")\n",
        "\n",
        "                # Save model info\n",
        "                model_info = {\n",
        "                    \"job_id\": job_id,\n",
        "                    \"fine_tuned_model\": job.fine_tuned_model,\n",
        "                    \"training_file\": training_file_id,\n",
        "                    \"validation_file\": validation_file_id,\n",
        "                    \"completed_at\": datetime.now().isoformat(),\n",
        "                    \"training_time_minutes\": elapsed/60\n",
        "                }\n",
        "\n",
        "                # Save to file\n",
        "                with open(f\"fine_tuned_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\", 'w') as f:\n",
        "                    json.dump(model_info, f, indent=2)\n",
        "\n",
        "                print(f\"   Model info saved to file\")\n",
        "                return job.fine_tuned_model\n",
        "\n",
        "            elif job.status == \"failed\":\n",
        "                print(f\"\\n❌ TRAINING FAILED\")\n",
        "                if hasattr(job, 'error') and job.error:\n",
        "                    print(f\"   Error: {job.error}\")\n",
        "                return None\n",
        "\n",
        "            elif job.status == \"cancelled\":\n",
        "                print(f\"\\n⚠️ TRAINING CANCELLED\")\n",
        "                return None\n",
        "\n",
        "            # Wait before next check\n",
        "            time.sleep(30)  # Check every 30 seconds\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"\\n⚠️ Monitoring interrupted by user\")\n",
        "        print(f\"   Job {job_id} is still running in the background\")\n",
        "        print(f\"   Use check_job_status('{job_id}') to check status later\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Monitoring error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==========================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def check_job_status(job_id_to_check):\n",
        "    \"\"\"Check status of a specific job\"\"\"\n",
        "    try:\n",
        "        job = client.fine_tuning.jobs.retrieve(job_id_to_check)\n",
        "\n",
        "        print(f\"\\n📊 JOB STATUS CHECK\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Job ID: {job_id_to_check}\")\n",
        "        print(f\"Status: {job.status}\")\n",
        "        print(f\"Model: {job.model}\")\n",
        "\n",
        "        if hasattr(job, 'fine_tuned_model') and job.fine_tuned_model:\n",
        "            print(f\"Fine-tuned model: {job.fine_tuned_model}\")\n",
        "\n",
        "        if hasattr(job, 'trained_tokens') and job.trained_tokens:\n",
        "            print(f\"Trained tokens: {job.trained_tokens:,}\")\n",
        "\n",
        "        return job\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking job status: {e}\")\n",
        "        return None\n",
        "\n",
        "def list_my_finetuning_jobs():\n",
        "    \"\"\"List all fine-tuning jobs for your account\"\"\"\n",
        "    try:\n",
        "        jobs = client.fine_tuning.jobs.list()\n",
        "\n",
        "        print(f\"\\n📋 YOUR FINE-TUNING JOBS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for job in jobs.data:\n",
        "            status_icon = \"✅\" if job.status == \"succeeded\" else \"⏳\" if job.status == \"running\" else \"❌\"\n",
        "            print(f\"{status_icon} {job.id} | {job.status} | {job.model}\")\n",
        "            if hasattr(job, 'fine_tuned_model') and job.fine_tuned_model:\n",
        "                print(f\"   → {job.fine_tuned_model}\")\n",
        "\n",
        "        return jobs.data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error listing jobs: {e}\")\n",
        "        return None\n",
        "\n",
        "def test_fine_tuned_model(model_name, test_subject=\"Urgent Account Verification\",\n",
        "                         test_body=\"Your account will be suspended unless you verify immediately. Click here: http://suspicious-link.com\"):\n",
        "    \"\"\"Test a fine-tuned model\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n🧪 TESTING FINE-TUNED MODEL\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Model: {model_name}\")\n",
        "\n",
        "        system_prompt = \"\"\"You are an expert cybersecurity analyst specializing in email threat detection. Your task is to analyze emails for phishing indicators using a structured approach.\n",
        "\n",
        "ANALYSIS FRAMEWORK:\n",
        "1. SENDER ANALYSIS: Examine sender authenticity, domain reputation, email authentication indicators\n",
        "2. LANGUAGE PATTERNS: Analyze grammar, writing style, urgency indicators, linguistic manipulation\n",
        "3. SOCIAL ENGINEERING: Identify psychological manipulation, emotional triggers, deceptive tactics\n",
        "4. TECHNICAL INDICATORS: Assess URLs, attachments, technical attack vectors, suspicious elements\n",
        "5. RISK ASSESSMENT: Synthesize findings and provide evidence-based classification\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "- Provide detailed analysis for each component\n",
        "- End with \"**FINAL CLASSIFICATION:** LEGITIMATE\" or \"**FINAL CLASSIFICATION:** PHISHING\"\n",
        "- Be specific about evidence and reasoning\n",
        "\n",
        "Your analysis should be thorough, evidence-based, and follow cybersecurity best practices.\"\"\"\n",
        "\n",
        "        test_messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Please analyze this email for phishing indicators:\n",
        "\n",
        "SUBJECT: {test_subject}\n",
        "\n",
        "BODY: {test_body}\n",
        "\n",
        "Provide detailed analysis through all five components and your final classification.\"\"\"}\n",
        "        ]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=test_messages,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "        print(f\"✅ Test successful!\")\n",
        "        print(f\"\\n📧 Test Email:\")\n",
        "        print(f\"Subject: {test_subject}\")\n",
        "        print(f\"Body: {test_body}\")\n",
        "        print(f\"\\n🤖 Model Response:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(response.choices[0].message.content)\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Test failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==========================================\n",
        "# MAIN EXECUTION FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def run_complete_workflow():\n",
        "    \"\"\"Run the complete workflow: upload → create job → monitor\"\"\"\n",
        "    print(\"🚀 RUNNING COMPLETE FINE-TUNING WORKFLOW\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Setup client\n",
        "    if not setup_client():\n",
        "        return False\n",
        "\n",
        "    # Step 1: Upload files\n",
        "    if not upload_files():\n",
        "        print(\"❌ Upload failed. Cannot proceed.\")\n",
        "        return False\n",
        "\n",
        "    # Step 2: Create job\n",
        "    if not create_finetuning_job():\n",
        "        print(\"❌ Job creation failed. Cannot proceed.\")\n",
        "        return False\n",
        "\n",
        "    # Step 3: Monitor training\n",
        "    fine_tuned_model = monitor_training()\n",
        "\n",
        "    if fine_tuned_model:\n",
        "        print(f\"\\n🎉 WORKFLOW COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\"   Your fine-tuned model: {fine_tuned_model}\")\n",
        "        print(f\"   Test it with: test_fine_tuned_model('{fine_tuned_model}')\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"\\n⚠️ Workflow incomplete. Check job status later.\")\n",
        "        return False\n",
        "\n",
        "def run_upload_only():\n",
        "    \"\"\"Run only file upload\"\"\"\n",
        "    if not setup_client():\n",
        "        return False\n",
        "    return upload_files()\n",
        "\n",
        "def run_create_job_only():\n",
        "    \"\"\"Create job with existing file IDs\"\"\"\n",
        "    # You need to set these manually if files are already uploaded\n",
        "    global training_file_id, validation_file_id\n",
        "    training_file_id = \"file-your-training-id\"  # Replace with actual ID\n",
        "    validation_file_id = \"file-your-validation-id\"  # Replace with actual ID\n",
        "\n",
        "    if not setup_client():\n",
        "        return False\n",
        "    return create_finetuning_job()\n",
        "\n",
        "def run_monitor_only():\n",
        "    \"\"\"Monitor existing job\"\"\"\n",
        "    global job_id\n",
        "    job_id = \"ft-your-job-id\"  # Replace with actual job ID\n",
        "\n",
        "    if not setup_client():\n",
        "        return False\n",
        "    return monitor_training()\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Available functions:\")\n",
        "    print(\"1. run_complete_workflow() - Complete pipeline\")\n",
        "    print(\"2. run_upload_only() - Upload files only\")\n",
        "    print(\"3. check_job_status('job-id') - Check specific job\")\n",
        "    print(\"4. list_my_finetuning_jobs() - List all your jobs\")\n",
        "    print(\"5. test_fine_tuned_model('model-name') - Test model\")\n",
        "    print(\"\\n⚠️  Remember to update API_KEY and file paths!\")\n",
        "\n",
        "# Uncomment to run complete workflow\n",
        "run_complete_workflow()"
      ],
      "metadata": {
        "id": "ujHCi__WBnHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete Model Testing and Evaluation\n",
        "Test fine-tuned GPT-4o mini on entire dataset and compare with original labels\n",
        "\n",
        "Usage: Test the fine-tuned model against the original dataset to measure performance\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from openai import OpenAI\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "# Your fine-tuned model name\n",
        "FINE_TUNED_MODEL = \"ft:gpt-4o-mini-2024-07-18:personal:phishing-detection:Btri5cqI\"\n",
        "\n",
        "# OpenAI API Key\n",
        "API_KEY = \"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\"\n",
        "\n",
        "# Dataset path\n",
        "DATASET_PATH = \"/content/drive/MyDrive/phishing_detection_final/output/final_datasets/o3_mini_optimized_dataset_20250709_185906.csv\"\n",
        "\n",
        "# Testing configuration\n",
        "MAX_EMAILS_TO_TEST = 1000  # Set to None for entire dataset\n",
        "BATCH_SIZE = 50  # Process in batches to avoid rate limits\n",
        "DELAY_BETWEEN_REQUESTS = 1  # Seconds between API calls\n",
        "\n",
        "print(\"🧪 COMPLETE MODEL TESTING AND EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"📊 Model: {FINE_TUNED_MODEL}\")\n",
        "print(f\"📂 Dataset: {DATASET_PATH}\")\n",
        "\n",
        "class ModelTester:\n",
        "    \"\"\"Complete testing framework for fine-tuned model\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, api_key, dataset_path):\n",
        "        self.model_name = model_name\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.dataset_path = dataset_path\n",
        "        self.df = None\n",
        "\n",
        "        # Results storage\n",
        "        self.predictions = []\n",
        "        self.true_labels = []\n",
        "        self.prediction_details = []\n",
        "        self.failed_predictions = 0\n",
        "\n",
        "        # Statistics\n",
        "        self.start_time = None\n",
        "        self.total_requests = 0\n",
        "        self.successful_requests = 0\n",
        "\n",
        "    def load_dataset(self, max_samples=None):\n",
        "        \"\"\"Load and prepare dataset for testing\"\"\"\n",
        "        print(f\"\\n📂 LOADING DATASET\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            self.df = pd.read_csv(self.dataset_path, encoding='utf-8')\n",
        "            print(f\"✅ Dataset loaded: {len(self.df):,} total rows\")\n",
        "\n",
        "            # Filter for required columns\n",
        "            required_cols = ['subject', 'body', 'original_label']\n",
        "            mask = self.df[required_cols].notna().all(axis=1)\n",
        "            self.df = self.df[mask].copy()\n",
        "\n",
        "            print(f\"📊 Clean examples: {len(self.df):,}\")\n",
        "\n",
        "            # Limit dataset size if specified\n",
        "            if max_samples and len(self.df) > max_samples:\n",
        "                # Stratified sampling to maintain class balance\n",
        "                legitimate_df = self.df[self.df['original_label'] == 0]\n",
        "                phishing_df = self.df[self.df['original_label'] == 1]\n",
        "\n",
        "                samples_per_class = max_samples // 2\n",
        "\n",
        "                legit_sample = legitimate_df.sample(n=min(samples_per_class, len(legitimate_df)), random_state=42)\n",
        "                phish_sample = phishing_df.sample(n=min(samples_per_class, len(phishing_df)), random_state=42)\n",
        "\n",
        "                self.df = pd.concat([legit_sample, phish_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "                print(f\"🎯 Limited to: {len(self.df):,} examples for testing\")\n",
        "\n",
        "            # Show class distribution\n",
        "            label_dist = self.df['original_label'].value_counts()\n",
        "            print(f\"\\n📊 Class Distribution:\")\n",
        "            print(f\"   Legitimate (0): {label_dist.get(0, 0):,}\")\n",
        "            print(f\"   Phishing (1):   {label_dist.get(1, 0):,}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading dataset: {e}\")\n",
        "            return False\n",
        "\n",
        "    def create_system_prompt(self):\n",
        "        \"\"\"System prompt for the fine-tuned model\"\"\"\n",
        "        return \"\"\"You are an expert cybersecurity analyst specializing in email threat detection. Your task is to analyze emails for phishing indicators using a structured approach.\n",
        "\n",
        "ANALYSIS FRAMEWORK:\n",
        "1. SENDER ANALYSIS: Examine sender authenticity, domain reputation, email authentication indicators\n",
        "2. LANGUAGE PATTERNS: Analyze grammar, writing style, urgency indicators, linguistic manipulation\n",
        "3. SOCIAL ENGINEERING: Identify psychological manipulation, emotional triggers, deceptive tactics\n",
        "4. TECHNICAL INDICATORS: Assess URLs, attachments, technical attack vectors, suspicious elements\n",
        "5. RISK ASSESSMENT: Synthesize findings and provide evidence-based classification\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "- Provide detailed analysis for each component\n",
        "- End with \"**FINAL CLASSIFICATION:** LEGITIMATE\" or \"**FINAL CLASSIFICATION:** PHISHING\"\n",
        "- Be specific about evidence and reasoning\n",
        "\n",
        "Your analysis should be thorough, evidence-based, and follow cybersecurity best practices.\"\"\"\n",
        "\n",
        "    def extract_prediction(self, response_text):\n",
        "        \"\"\"Extract prediction from model response\"\"\"\n",
        "        try:\n",
        "            # Look for final classification\n",
        "            patterns = [\n",
        "                r\"\\*\\*FINAL CLASSIFICATION:\\*\\*\\s*(LEGITIMATE|PHISHING)\",\n",
        "                r\"FINAL CLASSIFICATION:\\s*(LEGITIMATE|PHISHING)\",\n",
        "                r\"Classification:\\s*(LEGITIMATE|PHISHING)\",\n",
        "                r\"CLASSIFICATION:\\s*(LEGITIMATE|PHISHING)\"\n",
        "            ]\n",
        "\n",
        "            response_upper = response_text.upper()\n",
        "\n",
        "            for pattern in patterns:\n",
        "                match = re.search(pattern, response_upper)\n",
        "                if match:\n",
        "                    prediction = match.group(1)\n",
        "                    return 0 if prediction == \"LEGITIMATE\" else 1\n",
        "\n",
        "            # Fallback: look for keywords\n",
        "            if \"PHISHING\" in response_upper and \"LEGITIMATE\" not in response_upper:\n",
        "                return 1\n",
        "            elif \"LEGITIMATE\" in response_upper and \"PHISHING\" not in response_upper:\n",
        "                return 0\n",
        "\n",
        "            # If unclear, return None\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error extracting prediction: {e}\")\n",
        "            return None\n",
        "\n",
        "    def predict_single_email(self, subject, body, email_index):\n",
        "        \"\"\"Get prediction for a single email\"\"\"\n",
        "        try:\n",
        "            user_content = f\"\"\"Please analyze this email for phishing indicators:\n",
        "\n",
        "SUBJECT: {subject}\n",
        "\n",
        "BODY: {body}\n",
        "\n",
        "Provide detailed analysis through all five components and your final classification.\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": self.create_system_prompt()},\n",
        "                {\"role\": \"user\", \"content\": user_content}\n",
        "            ]\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=messages,\n",
        "                temperature=0.1,\n",
        "                max_tokens=2000\n",
        "            )\n",
        "\n",
        "            response_text = response.choices[0].message.content\n",
        "            prediction = self.extract_prediction(response_text)\n",
        "\n",
        "            self.total_requests += 1\n",
        "\n",
        "            if prediction is not None:\n",
        "                self.successful_requests += 1\n",
        "                return prediction, response_text\n",
        "            else:\n",
        "                print(f\"⚠️ Could not extract prediction from email {email_index}\")\n",
        "                return None, response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error predicting email {email_index}: {e}\")\n",
        "            self.total_requests += 1\n",
        "            return None, str(e)\n",
        "\n",
        "    def test_model_on_dataset(self):\n",
        "        \"\"\"Test model on entire dataset\"\"\"\n",
        "        print(f\"\\n🧪 TESTING MODEL ON DATASET\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"📊 Testing {len(self.df):,} emails\")\n",
        "        print(f\"⏱️ Estimated time: {(len(self.df) * DELAY_BETWEEN_REQUESTS) / 60:.1f} minutes\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        # Process in batches\n",
        "        for batch_start in range(0, len(self.df), BATCH_SIZE):\n",
        "            batch_end = min(batch_start + BATCH_SIZE, len(self.df))\n",
        "            batch_df = self.df.iloc[batch_start:batch_end]\n",
        "\n",
        "            print(f\"📦 Processing batch {batch_start//BATCH_SIZE + 1}: emails {batch_start+1}-{batch_end}\")\n",
        "\n",
        "            for idx, row in batch_df.iterrows():\n",
        "                try:\n",
        "                    subject = str(row.get('subject', 'No Subject')).strip()\n",
        "                    body = str(row.get('body', '')).strip()\n",
        "                    true_label = int(row.get('original_label', 0))\n",
        "\n",
        "                    # Get prediction\n",
        "                    prediction, response_text = self.predict_single_email(subject, body, idx)\n",
        "\n",
        "                    # Store results\n",
        "                    self.true_labels.append(true_label)\n",
        "\n",
        "                    if prediction is not None:\n",
        "                        self.predictions.append(prediction)\n",
        "\n",
        "                        # Store detailed results\n",
        "                        self.prediction_details.append({\n",
        "                            'email_index': idx,\n",
        "                            'subject': subject[:100],  # Truncate for storage\n",
        "                            'true_label': true_label,\n",
        "                            'predicted_label': prediction,\n",
        "                            'correct': prediction == true_label,\n",
        "                            'response_text': response_text[:500],  # Truncate\n",
        "                            'timestamp': datetime.now().isoformat()\n",
        "                        })\n",
        "                    else:\n",
        "                        self.predictions.append(-1)  # Mark as failed\n",
        "                        self.failed_predictions += 1\n",
        "\n",
        "                    # Progress update\n",
        "                    if (idx + 1) % 10 == 0:\n",
        "                        elapsed = time.time() - self.start_time\n",
        "                        processed = len(self.predictions)\n",
        "                        rate = processed / elapsed if elapsed > 0 else 0\n",
        "                        print(f\"   Processed: {processed:,}/{len(self.df):,} ({rate:.1f} emails/sec)\")\n",
        "\n",
        "                    # Rate limiting\n",
        "                    time.sleep(DELAY_BETWEEN_REQUESTS)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error processing email {idx}: {e}\")\n",
        "                    self.true_labels.append(int(row.get('original_label', 0)))\n",
        "                    self.predictions.append(-1)\n",
        "                    self.failed_predictions += 1\n",
        "                    continue\n",
        "\n",
        "        total_time = time.time() - self.start_time\n",
        "\n",
        "        print(f\"\\n✅ TESTING COMPLETED\")\n",
        "        print(f\"   Total time: {total_time/60:.1f} minutes\")\n",
        "        print(f\"   Total emails: {len(self.df):,}\")\n",
        "        print(f\"   Successful predictions: {self.successful_requests:,}\")\n",
        "        print(f\"   Failed predictions: {self.failed_predictions:,}\")\n",
        "        print(f\"   Success rate: {(self.successful_requests/len(self.df)*100):.1f}%\")\n",
        "\n",
        "    def calculate_metrics(self):\n",
        "        \"\"\"Calculate performance metrics\"\"\"\n",
        "        print(f\"\\n📊 PERFORMANCE METRICS\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Filter out failed predictions\n",
        "        valid_indices = [i for i, pred in enumerate(self.predictions) if pred != -1]\n",
        "        valid_predictions = [self.predictions[i] for i in valid_indices]\n",
        "        valid_true_labels = [self.true_labels[i] for i in valid_indices]\n",
        "\n",
        "        if not valid_predictions:\n",
        "            print(\"❌ No valid predictions to evaluate\")\n",
        "            return\n",
        "\n",
        "        # Basic counts\n",
        "        total_valid = len(valid_predictions)\n",
        "        correct_predictions = sum(1 for i in range(total_valid) if valid_predictions[i] == valid_true_labels[i])\n",
        "        accuracy = correct_predictions / total_valid\n",
        "\n",
        "        print(f\"📈 OVERALL PERFORMANCE:\")\n",
        "        print(f\"   Total valid predictions: {total_valid:,}\")\n",
        "        print(f\"   Correct predictions: {correct_predictions:,}\")\n",
        "        print(f\"   Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
        "\n",
        "        # Confusion Matrix\n",
        "        tp = sum(1 for i in range(total_valid) if valid_true_labels[i] == 1 and valid_predictions[i] == 1)\n",
        "        tn = sum(1 for i in range(total_valid) if valid_true_labels[i] == 0 and valid_predictions[i] == 0)\n",
        "        fp = sum(1 for i in range(total_valid) if valid_true_labels[i] == 0 and valid_predictions[i] == 1)\n",
        "        fn = sum(1 for i in range(total_valid) if valid_true_labels[i] == 1 and valid_predictions[i] == 0)\n",
        "\n",
        "        print(f\"\\n📊 CONFUSION MATRIX:\")\n",
        "        print(f\"                 Predicted\")\n",
        "        print(f\"                 Legit  Phish\")\n",
        "        print(f\"   Actual Legit   {tn:4d}   {fp:4d}\")\n",
        "        print(f\"          Phish   {fn:4d}   {tp:4d}\")\n",
        "\n",
        "        # Detailed metrics\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        print(f\"\\n📏 DETAILED METRICS:\")\n",
        "        print(f\"   Precision: {precision:.3f} ({precision*100:.1f}%)\")\n",
        "        print(f\"   Recall:    {recall:.3f} ({recall*100:.1f}%)\")\n",
        "        print(f\"   F1-Score:  {f1_score:.3f}\")\n",
        "\n",
        "        # Class-specific accuracy\n",
        "        legit_total = sum(1 for label in valid_true_labels if label == 0)\n",
        "        phish_total = sum(1 for label in valid_true_labels if label == 1)\n",
        "\n",
        "        legit_correct = tn\n",
        "        phish_correct = tp\n",
        "\n",
        "        legit_accuracy = legit_correct / legit_total if legit_total > 0 else 0\n",
        "        phish_accuracy = phish_correct / phish_total if phish_total > 0 else 0\n",
        "\n",
        "        print(f\"\\n🎯 CLASS-SPECIFIC ACCURACY:\")\n",
        "        print(f\"   Legitimate emails: {legit_correct:,}/{legit_total:,} ({legit_accuracy*100:.1f}%)\")\n",
        "        print(f\"   Phishing emails:   {phish_correct:,}/{phish_total:,} ({phish_accuracy*100:.1f}%)\")\n",
        "\n",
        "        # Label counts\n",
        "        true_legit = sum(1 for label in valid_true_labels if label == 0)\n",
        "        true_phish = sum(1 for label in valid_true_labels if label == 1)\n",
        "        pred_legit = sum(1 for label in valid_predictions if label == 0)\n",
        "        pred_phish = sum(1 for label in valid_predictions if label == 1)\n",
        "\n",
        "        print(f\"\\n📊 LABEL DISTRIBUTION:\")\n",
        "        print(f\"   True Labels  - Legitimate: {true_legit:,}, Phishing: {true_phish:,}\")\n",
        "        print(f\"   Predictions  - Legitimate: {pred_legit:,}, Phishing: {pred_phish:,}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1_score,\n",
        "            'confusion_matrix': {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn},\n",
        "            'total_valid': total_valid,\n",
        "            'correct_predictions': correct_predictions\n",
        "        }\n",
        "\n",
        "    def show_sample_predictions(self, num_correct=3, num_incorrect=3):\n",
        "        \"\"\"Show sample correct and incorrect predictions\"\"\"\n",
        "        print(f\"\\n🔍 SAMPLE PREDICTIONS\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Get correct and incorrect predictions\n",
        "        correct_samples = []\n",
        "        incorrect_samples = []\n",
        "\n",
        "        for detail in self.prediction_details:\n",
        "            if detail['correct'] and len(correct_samples) < num_correct:\n",
        "                correct_samples.append(detail)\n",
        "            elif not detail['correct'] and len(incorrect_samples) < num_incorrect:\n",
        "                incorrect_samples.append(detail)\n",
        "\n",
        "        # Show correct predictions\n",
        "        if correct_samples:\n",
        "            print(f\"✅ CORRECT PREDICTIONS (showing {len(correct_samples)}):\")\n",
        "            for i, sample in enumerate(correct_samples, 1):\n",
        "                true_label_name = \"LEGITIMATE\" if sample['true_label'] == 0 else \"PHISHING\"\n",
        "                pred_label_name = \"LEGITIMATE\" if sample['predicted_label'] == 0 else \"PHISHING\"\n",
        "                print(f\"\\n   {i}. Subject: {sample['subject']}\")\n",
        "                print(f\"      True: {true_label_name} | Predicted: {pred_label_name} ✅\")\n",
        "\n",
        "        # Show incorrect predictions\n",
        "        if incorrect_samples:\n",
        "            print(f\"\\n❌ INCORRECT PREDICTIONS (showing {len(incorrect_samples)}):\")\n",
        "            for i, sample in enumerate(incorrect_samples, 1):\n",
        "                true_label_name = \"LEGITIMATE\" if sample['true_label'] == 0 else \"PHISHING\"\n",
        "                pred_label_name = \"LEGITIMATE\" if sample['predicted_label'] == 0 else \"PHISHING\"\n",
        "                print(f\"\\n   {i}. Subject: {sample['subject']}\")\n",
        "                print(f\"      True: {true_label_name} | Predicted: {pred_label_name} ❌\")\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save detailed results to files\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Save detailed results\n",
        "        results_file = f\"model_evaluation_results_{timestamp}.json\"\n",
        "\n",
        "        results_data = {\n",
        "            'model_name': self.model_name,\n",
        "            'dataset_path': self.dataset_path,\n",
        "            'test_timestamp': timestamp,\n",
        "            'total_emails_tested': len(self.df),\n",
        "            'successful_predictions': self.successful_requests,\n",
        "            'failed_predictions': self.failed_predictions,\n",
        "            'metrics': self.calculate_metrics(),\n",
        "            'sample_predictions': self.prediction_details[:50]  # Save first 50 for inspection\n",
        "        }\n",
        "\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\n💾 RESULTS SAVED:\")\n",
        "        print(f\"   📄 Detailed results: {results_file}\")\n",
        "\n",
        "        # Save predictions CSV\n",
        "        if self.prediction_details:\n",
        "            predictions_df = pd.DataFrame(self.prediction_details)\n",
        "            csv_file = f\"model_predictions_{timestamp}.csv\"\n",
        "            predictions_df.to_csv(csv_file, index=False)\n",
        "            print(f\"   📊 Predictions CSV: {csv_file}\")\n",
        "\n",
        "        return results_file\n",
        "\n",
        "def run_complete_evaluation():\n",
        "    \"\"\"Run complete model evaluation\"\"\"\n",
        "\n",
        "    # Initialize tester\n",
        "    tester = ModelTester(FINE_TUNED_MODEL, API_KEY, DATASET_PATH)\n",
        "\n",
        "    # Load dataset\n",
        "    if not tester.load_dataset(max_samples=MAX_EMAILS_TO_TEST):\n",
        "        return False\n",
        "\n",
        "    # Test model\n",
        "    tester.test_model_on_dataset()\n",
        "\n",
        "    # Calculate and show metrics\n",
        "    metrics = tester.calculate_metrics()\n",
        "\n",
        "    # Show sample predictions\n",
        "    tester.show_sample_predictions()\n",
        "\n",
        "    # Save results\n",
        "    results_file = tester.save_results()\n",
        "\n",
        "    print(f\"\\n🎉 EVALUATION COMPLETED!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"📊 Final Results:\")\n",
        "    print(f\"   Accuracy: {metrics['accuracy']*100:.1f}%\")\n",
        "    print(f\"   Precision: {metrics['precision']*100:.1f}%\")\n",
        "    print(f\"   Recall: {metrics['recall']*100:.1f}%\")\n",
        "    print(f\"   F1-Score: {metrics['f1_score']:.3f}\")\n",
        "    print(f\"   Results saved to: {results_file}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def quick_test(num_samples=50):\n",
        "    \"\"\"Quick test with limited samples\"\"\"\n",
        "    global MAX_EMAILS_TO_TEST\n",
        "    MAX_EMAILS_TO_TEST = num_samples\n",
        "\n",
        "    print(f\"🚀 QUICK TEST MODE: Testing {num_samples} emails\")\n",
        "    return run_complete_evaluation()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Available functions:\")\n",
        "    print(\"1. run_complete_evaluation() - Test entire dataset\")\n",
        "    print(\"2. quick_test(50) - Quick test with 50 emails\")\n",
        "    print(\"3. quick_test(100) - Quick test with 100 emails\")\n",
        "\n",
        "# Run complete evaluation\n",
        "run_complete_evaluation()"
      ],
      "metadata": {
        "id": "89nw-iOBaQZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}