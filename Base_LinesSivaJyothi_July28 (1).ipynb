{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rEnuxPygHuX"
      },
      "outputs": [],
      "source": [
        "def generate_final_results_table(self):\n",
        "        \"\"\"Generate final comprehensive results table\"\"\"\n",
        "        print(\"\\nðŸ“Š FINAL COMPREHENSIVE RESULTS TABLE\")\n",
        "        print(\"=\" * 120)\n",
        "\n",
        "        # Collect all results in a structured format\n",
        "        all_results = []\n",
        "\n",
        "        for config_name, models in self.results.items():\n",
        "            features_used = \"Yes\" if \"With\" in config_name else \"No\"\n",
        "\n",
        "            for model_name, metrics in models.items():\n",
        "                if model_name in MODELS_CONFIG:\n",
        "                    # Calculate combined error rate\n",
        "                    combined_error = metrics['false_positive_rate'] + metrics['false_negative_rate']\n",
        "\n",
        "                    all_results.append({\n",
        "                        'Features': features_used,\n",
        "                        'Model': MODELS_CONFIG[model_name]['name'],\n",
        "                        'Accuracy': f\"{metrics['accuracy']:.3f}\",\n",
        "                        'Legit F1': f\"{metrics['legitimate_f1']:.3f}\",\n",
        "                        'Phish F1': f\"{metrics['phishing_f1']:.3f}\",\n",
        "                        'Avg F1': f\"{metrics['f1_score']:.3f}\",\n",
        "                        'FPR': f\"{metrics['false_positive_rate']:.3f}\",\n",
        "                        'FNR': f\"{metrics['false_negative_rate']:.3f}\",\n",
        "                        'FPR+FNR': f\"{combined_error:.3f}\",\n",
        "                        'Time(s)': f\"{metrics['train_time']:.1f}\"\n",
        "                    })\n",
        "\n",
        "        # Create DataFrame and display\n",
        "        df_final = pd.DataFrame(all_results)\n",
        "\n",
        "        # Sort by Features (No first) and then by combined error rate\n",
        "        df_final['sort_features'] = df_final['Features'].map({'No': 0, 'Yes': 1})\n",
        "        df_final['sort_error'] = df_final['FPR+FNR'].astype(float)\n",
        "        df_final = df_final.sort_values(['sort_features', 'sort_error']).drop(['sort_features', 'sort_error'], axis=1)\n",
        "\n",
        "        print(df_final.to_string(index=False))\n",
        "\n",
        "        # Best models summary\n",
        "        print(\"\\nðŸ† BEST MODELS BY METRIC\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Without features\n",
        "        no_features_df = df_final[df_final['Features'] == 'No']\n",
        "        if not no_features_df.empty:\n",
        "            print(\"\\nWithout Additional Features:\")\n",
        "            best_f1 = no_features_df.loc[no_features_df['Avg F1'].astype(float).idxmax()]\n",
        "            print(f\"  Best F1-Score: {best_f1['Model']} ({best_f1['Avg F1']})\")\n",
        "\n",
        "            best_balanced = no_features_df.loc[no_features_df['FPR+FNR'].astype(float).idxmin()]\n",
        "            print(f\"  Best Balanced: {best_balanced['Model']} (FPR+FNR: {best_balanced['FPR+FNR']})\")\n",
        "\n",
        "        # With features\n",
        "        yes_features_df = df_final[df_final['Features'] == 'Yes']\n",
        "        if not yes_features_df.empty:\n",
        "            print(\"\\nWith Additional Features:\")\n",
        "            best_f1 = yes_features_df.loc[yes_features_df['Avg F1'].astype(float).idxmax()]\n",
        "            print(f\"  Best F1-Score: {best_f1['Model']} ({best_f1['Avg F1']})\")\n",
        "\n",
        "            best_balanced = yes_features_df.loc[yes_features_df['FPR+FNR'].astype(float).idxmin()]\n",
        "            print(f\"  Best Balanced: {best_balanced['Model']} (FPR+FNR: {best_balanced['FPR+FNR']})\")\n",
        "\n",
        "        # Key insights\n",
        "        print(\"\\nðŸ“ˆ KEY PERFORMANCE INSIGHTS\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Compare feature impact\n",
        "        print(\"\\n1. Impact of Additional Features:\")\n",
        "        for model in ['Logistic Regression Baseline', 'Random Forest Baseline', 'XGBoost Baseline', 'BERT Base Uncased']:\n",
        "            no_feat = df_final[(df_final['Features'] == 'No') & (df_final['Model'] == model)]\n",
        "            yes_feat = df_final[(df_final['Features'] == 'Yes') & (df_final['Model'] == model)]\n",
        "\n",
        "            if not no_feat.empty and not yes_feat.empty:\n",
        "                no_f1 = float(no_feat.iloc[0]['Avg F1'])\n",
        "                yes_f1 = float(yes_feat.iloc[0]['Avg F1'])\n",
        "                diff = yes_f1 - no_f1\n",
        "\n",
        "                if diff != 0:  # Only show if there's a difference\n",
        "                    print(f\"   {model}: {'+' if diff > 0 else ''}{diff:.3f} F1 improvement\")\n",
        "\n",
        "        # Speed vs Performance trade-off\n",
        "        print(\"\\n2. Speed vs Performance Trade-off:\")\n",
        "        print(\"   Fast (<1s) + High Performance (F1>0.97):\")\n",
        "        fast_good = df_final[(df_final['Time(s)'].astype(float) < 1) & (df_final['Avg F1'].astype(float) > 0.97)]\n",
        "        for _, row in fast_good.iterrows():\n",
        "            print(f\"   - {row['Model']} with {row['Features']} features: F1={row['Avg F1']}, Time={row['Time(s)']}s\")\n",
        "\n",
        "        # Class balance analysis\n",
        "        print(\"\\n3. Class Balance Analysis (FPR vs FNR):\")\n",
        "        for _, row in df_final.iterrows():\n",
        "            fpr = float(row['FPR'])\n",
        "            fnr = float(row['FNR'])\n",
        "            if abs(fpr - fnr) < 0.01:  # Well balanced\n",
        "                print(f\"   Well balanced: {row['Model']} with {row['Features']} features (FPR={row['FPR']}, FNR={row['FNR']})\")\n",
        "\n",
        "        # Save enhanced results\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        final_csv = f\"final_results_table_{timestamp}.csv\"\n",
        "        df_final.to_csv(final_csv, index=False)\n",
        "        print(f\"\\nðŸ’¾ Final results table saved to: {final_csv}\")\n",
        "\n",
        "        return df_final#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Multi-Model Phishing Detection Evaluation Framework\n",
        "Matches the exact sampling strategy used in fine-tuning:\n",
        "- 3,000 examples per class (6,000 total)\n",
        "- 4,730 training / 1,000 validation split\n",
        "\n",
        "GPU-enabled for faster processing\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION - MATCHING FINE-TUNING\n",
        "# ==========================================\n",
        "\n",
        "# Dataset path (same as your original)\n",
        "DATASET_PATH = \"/content/drive/MyDrive/phishing_detection_final/output/final_datasets/o3_mini_optimized_dataset_20250709_185906.csv\"\n",
        "\n",
        "# Sampling configuration (matching your fine-tuning)\n",
        "SAMPLES_PER_CLASS = 3000  # Same as fine-tuning\n",
        "VALIDATION_SIZE = 1000    # Same as fine-tuning\n",
        "MAX_TOKEN_LENGTH = 1591   # 95th percentile from your statistics\n",
        "\n",
        "# Model configurations\n",
        "MODELS_CONFIG = {\n",
        "    'baseline_lr': {\n",
        "        'name': 'Logistic Regression Baseline',\n",
        "        'type': 'sklearn'\n",
        "    },\n",
        "    'baseline_rf': {\n",
        "        'name': 'Random Forest Baseline',\n",
        "        'type': 'sklearn'\n",
        "    },\n",
        "    'baseline_xgb': {\n",
        "        'name': 'XGBoost Baseline',\n",
        "        'type': 'xgboost'\n",
        "    },\n",
        "    'bert': {\n",
        "        'name': 'BERT Base Uncased',\n",
        "        'model_name': 'bert-base-uncased',\n",
        "        'type': 'transformer'\n",
        "    },\n",
        "    'secbert': {\n",
        "        'name': 'SecBERT (jackaduma)',\n",
        "        'model_name': 'jackaduma/SecBERT',\n",
        "        'type': 'transformer'\n",
        "    },\n",
        "    'securebert': {\n",
        "        'name': 'SecureBERT (ehsanaghaei)',\n",
        "        'model_name': 'ehsanaghaei/SecureBERT',\n",
        "        'type': 'transformer'\n",
        "    },\n",
        "    'cysecbert': {\n",
        "        'name': 'CySecBERT (markusbayer)',\n",
        "        'model_name': 'markusbayer/CySecBERT',\n",
        "        'type': 'transformer'\n",
        "    },\n",
        "    'lstm': {\n",
        "        'name': 'BiLSTM',\n",
        "        'type': 'lstm'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Choose which cybersecurity models to use\n",
        "CYBER_MODELS_TO_USE = ['secbert']  # Add more as needed\n",
        "\n",
        "# Training configuration\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 512  # For transformers\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# GPU Configuration\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸ–¥ï¸ Using device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# ==========================================\n",
        "# DATASET HANDLING WITH MATCHED SAMPLING\n",
        "# ==========================================\n",
        "\n",
        "class PhishingDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for phishing detection\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512, additional_features=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.additional_features = additional_features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        if self.additional_features is not None:\n",
        "            item['additional_features'] = torch.tensor(\n",
        "                self.additional_features[idx],\n",
        "                dtype=torch.float\n",
        "            )\n",
        "\n",
        "        return item\n",
        "\n",
        "# ==========================================\n",
        "# MODEL DEFINITIONS (Same as before)\n",
        "# ==========================================\n",
        "\n",
        "class BiLSTMClassifier(nn.Module):\n",
        "    \"\"\"Bidirectional LSTM for text classification\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256,\n",
        "                 num_layers=2, dropout=0.3, num_classes=2,\n",
        "                 additional_features_dim=0):\n",
        "        super(BiLSTMClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        lstm_output_dim = hidden_dim * 2\n",
        "        final_dim = lstm_output_dim + additional_features_dim\n",
        "\n",
        "        self.fc = nn.Linear(final_dim, num_classes)\n",
        "        self.additional_features_dim = additional_features_dim\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, additional_features=None):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "\n",
        "        last_hidden = lstm_out[:, -1, :]\n",
        "\n",
        "        if additional_features is not None and self.additional_features_dim > 0:\n",
        "            last_hidden = torch.cat([last_hidden, additional_features], dim=1)\n",
        "\n",
        "        dropped = self.dropout(last_hidden)\n",
        "        output = self.fc(dropped)\n",
        "\n",
        "        return output\n",
        "\n",
        "class EnhancedBERTClassifier(nn.Module):\n",
        "    \"\"\"BERT with optional additional features\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, num_classes=2, additional_features_dim=0):\n",
        "        super(EnhancedBERTClassifier, self).__init__()\n",
        "\n",
        "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_classes\n",
        "        )\n",
        "\n",
        "        self.additional_features_dim = additional_features_dim\n",
        "\n",
        "        if additional_features_dim > 0:\n",
        "            bert_hidden_size = self.bert.config.hidden_size\n",
        "            self.bert.classifier = nn.Linear(\n",
        "                bert_hidden_size + additional_features_dim,\n",
        "                num_classes\n",
        "            )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, additional_features=None):\n",
        "        outputs = self.bert.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        if additional_features is not None and self.additional_features_dim > 0:\n",
        "            pooled_output = torch.cat([pooled_output, additional_features], dim=1)\n",
        "\n",
        "        logits = self.bert.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# ==========================================\n",
        "# EVALUATION FRAMEWORK WITH MATCHED SAMPLING\n",
        "# ==========================================\n",
        "\n",
        "class MatchedSamplingEvaluator:\n",
        "    \"\"\"Evaluation framework matching fine-tuning sampling strategy\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.df = None\n",
        "        self.results = {}\n",
        "        self.additional_feature_columns = []\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        \"\"\"Load dataset and apply same sampling as fine-tuning\"\"\"\n",
        "        print(\"ðŸ“‚ LOADING AND SAMPLING DATASET (Matching Fine-Tuning)\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Load full dataset\n",
        "        self.df = pd.read_csv(self.dataset_path)\n",
        "        print(f\"âœ… Dataset loaded: {len(self.df):,} total rows\")\n",
        "\n",
        "        # Identify columns\n",
        "        base_columns = ['subject', 'body', 'original_label']\n",
        "        all_columns = list(self.df.columns)\n",
        "\n",
        "        # Additional feature columns\n",
        "        self.additional_feature_columns = [\n",
        "            col for col in all_columns\n",
        "            if col not in base_columns and col != 'Unnamed: 0'\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nðŸ“Š Column Analysis:\")\n",
        "        print(f\"   Base columns: {base_columns}\")\n",
        "        print(f\"   Additional columns: {len(self.additional_feature_columns)}\")\n",
        "\n",
        "        # Create combined text\n",
        "        self.df['combined_text'] = self.df['subject'].fillna('') + ' ' + self.df['body'].fillna('')\n",
        "\n",
        "        # Clean data\n",
        "        self.df = self.df[self.df['combined_text'].str.strip() != '']\n",
        "        self.df = self.df[self.df['original_label'].notna()]\n",
        "\n",
        "        print(f\"\\nðŸ“Š Clean examples: {len(self.df):,}\")\n",
        "\n",
        "        # Split by class\n",
        "        legitimate_df = self.df[self.df['original_label'] == 0]\n",
        "        phishing_df = self.df[self.df['original_label'] == 1]\n",
        "\n",
        "        print(f\"   Legitimate: {len(legitimate_df):,}\")\n",
        "        print(f\"   Phishing: {len(phishing_df):,}\")\n",
        "\n",
        "        # Sample same as fine-tuning: 3,000 per class\n",
        "        print(f\"\\nðŸ“ Sampling {SAMPLES_PER_CLASS:,} examples per class\")\n",
        "\n",
        "        # Sample with same random state for reproducibility\n",
        "        legitimate_sample = legitimate_df.sample(\n",
        "            n=min(SAMPLES_PER_CLASS, len(legitimate_df)),\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "        phishing_sample = phishing_df.sample(\n",
        "            n=min(SAMPLES_PER_CLASS, len(phishing_df)),\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        # Combine samples\n",
        "        self.df = pd.concat([legitimate_sample, phishing_sample])\n",
        "        self.df = self.df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "        print(f\"\\nâœ… Total examples: {len(self.df):,}\")\n",
        "\n",
        "        # Show final distribution\n",
        "        class_dist = self.df['original_label'].value_counts().sort_index()\n",
        "        print(f\"\\nðŸ“Š Final Class Distribution:\")\n",
        "        print(f\"   Legitimate (0): {class_dist.get(0, 0):,}\")\n",
        "        print(f\"   Phishing (1):   {class_dist.get(1, 0):,}\")\n",
        "\n",
        "        # Token length filtering (optional - to match fine-tuning)\n",
        "        print(f\"\\nðŸ” Checking token lengths (for information only)\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def prepare_features(self, use_additional_features=True):\n",
        "        \"\"\"Prepare features for training\"\"\"\n",
        "        X_text = self.df['combined_text'].values\n",
        "        y = self.df['original_label'].values.astype(int)\n",
        "\n",
        "        X_additional = None\n",
        "        if use_additional_features and self.additional_feature_columns:\n",
        "            # Select numeric columns only\n",
        "            numeric_features = self.df[self.additional_feature_columns].select_dtypes(\n",
        "                include=[np.number]\n",
        "            ).columns.tolist()\n",
        "\n",
        "            if numeric_features:\n",
        "                X_additional = self.df[numeric_features].values\n",
        "\n",
        "                # Handle missing values\n",
        "                X_additional = np.nan_to_num(X_additional, 0)\n",
        "\n",
        "                # Standardize features\n",
        "                scaler = StandardScaler()\n",
        "                X_additional = scaler.fit_transform(X_additional)\n",
        "\n",
        "                print(f\"   Using {X_additional.shape[1]} additional features\")\n",
        "\n",
        "        return X_text, X_additional, y\n",
        "\n",
        "    def create_train_val_split(self, X_text, X_additional, y):\n",
        "        \"\"\"Create train/validation split matching fine-tuning\"\"\"\n",
        "        # Calculate sizes to match fine-tuning\n",
        "        total_samples = len(y)\n",
        "        val_size = VALIDATION_SIZE\n",
        "        train_size = total_samples - val_size\n",
        "\n",
        "        print(f\"\\nðŸ“Š CREATING TRAIN/VALIDATION SPLIT:\")\n",
        "        print(f\"   Total samples: {total_samples:,}\")\n",
        "        print(f\"   Training samples: {train_size:,}\")\n",
        "        print(f\"   Validation samples: {val_size:,}\")\n",
        "\n",
        "        # Stratified split\n",
        "        indices = np.arange(len(y))\n",
        "        train_idx, val_idx = train_test_split(\n",
        "            indices,\n",
        "            test_size=val_size/total_samples,\n",
        "            random_state=RANDOM_STATE,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        # Split text\n",
        "        X_train_text = X_text[train_idx]\n",
        "        X_val_text = X_text[val_idx]\n",
        "\n",
        "        # Split labels\n",
        "        y_train = y[train_idx]\n",
        "        y_val = y[val_idx]\n",
        "\n",
        "        # Split additional features if present\n",
        "        if X_additional is not None:\n",
        "            X_train_add = X_additional[train_idx]\n",
        "            X_val_add = X_additional[val_idx]\n",
        "        else:\n",
        "            X_train_add = None\n",
        "            X_val_add = None\n",
        "\n",
        "        # Verify class distribution\n",
        "        print(f\"\\n   Training set class distribution:\")\n",
        "        unique, counts = np.unique(y_train, return_counts=True)\n",
        "        for label, count in zip(unique, counts):\n",
        "            print(f\"      Class {label}: {count:,} ({count/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "        print(f\"\\n   Validation set class distribution:\")\n",
        "        unique, counts = np.unique(y_val, return_counts=True)\n",
        "        for label, count in zip(unique, counts):\n",
        "            print(f\"      Class {label}: {count:,} ({count/len(y_val)*100:.1f}%)\")\n",
        "\n",
        "        return (X_train_text, X_val_text, y_train, y_val, X_train_add, X_val_add)\n",
        "\n",
        "    def train_baseline_models(self, X_train_text, X_val_text, y_train, y_val,\n",
        "                            X_train_add, X_val_add, use_additional_features):\n",
        "        \"\"\"Train baseline ML models\"\"\"\n",
        "        print(\"\\nðŸ¤– TRAINING BASELINE MODELS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Create TF-IDF features\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
        "        X_val_tfidf = vectorizer.transform(X_val_text)\n",
        "\n",
        "        # Combine features if needed\n",
        "        if use_additional_features and X_train_add is not None:\n",
        "            from scipy.sparse import hstack\n",
        "            X_train_combined = hstack([X_train_tfidf, X_train_add])\n",
        "            X_val_combined = hstack([X_val_tfidf, X_val_add])\n",
        "        else:\n",
        "            X_train_combined = X_train_tfidf\n",
        "            X_val_combined = X_val_tfidf\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Train models\n",
        "        models = {\n",
        "            'baseline_lr': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, class_weight='balanced'),\n",
        "            'baseline_rf': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, class_weight='balanced'),\n",
        "            'baseline_xgb': xgb.XGBClassifier(\n",
        "                n_estimators=100,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=6,\n",
        "                random_state=RANDOM_STATE,\n",
        "                tree_method='gpu_hist' if torch.cuda.is_available() else 'auto',\n",
        "                scale_pos_weight=1,\n",
        "                objective='binary:logistic',\n",
        "                use_label_encoder=False,\n",
        "                eval_metric='logloss'\n",
        "            )\n",
        "        }\n",
        "\n",
        "        for model_key, model in models.items():\n",
        "            if model_key not in MODELS_CONFIG:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nðŸ“Š Training {MODELS_CONFIG[model_key]['name']}...\")\n",
        "\n",
        "            start_time = time.time()\n",
        "            model.fit(X_train_combined, y_train)\n",
        "            train_time = time.time() - start_time\n",
        "\n",
        "            # Predict on validation set\n",
        "            y_pred = model.predict(X_val_combined)\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(y_val, y_pred)\n",
        "            precision, recall, f1, support = precision_recall_fscore_support(\n",
        "                y_val, y_pred, average=None, labels=[0, 1]\n",
        "            )\n",
        "            precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(\n",
        "                y_val, y_pred, average='binary'\n",
        "            )\n",
        "            cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "            # Calculate additional metrics\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
        "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
        "\n",
        "            results[model_key] = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision_avg,\n",
        "                'recall': recall_avg,\n",
        "                'f1_score': f1_avg,\n",
        "                'confusion_matrix': cm.tolist(),\n",
        "                # Class-wise metrics\n",
        "                'legitimate_precision': precision[0],\n",
        "                'legitimate_recall': recall[0],\n",
        "                'legitimate_f1': f1[0],\n",
        "                'legitimate_support': int(support[0]),\n",
        "                'phishing_precision': precision[1],\n",
        "                'phishing_recall': recall[1],\n",
        "                'phishing_f1': f1[1],\n",
        "                'phishing_support': int(support[1]),\n",
        "                # Error rates\n",
        "                'false_positive_rate': fpr,\n",
        "                'false_negative_rate': fnr,\n",
        "                'true_negatives': int(tn),\n",
        "                'false_positives': int(fp),\n",
        "                'false_negatives': int(fn),\n",
        "                'true_positives': int(tp),\n",
        "                'train_time': train_time,\n",
        "                'val_size': len(y_val),\n",
        "                'train_size': len(y_train)\n",
        "            }\n",
        "\n",
        "            print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "            print(f\"   F1-Score: {f1_avg:.3f}\")\n",
        "            print(f\"   Training time: {train_time:.1f}s\")\n",
        "\n",
        "            # Print class-wise summary\n",
        "            print(f\"   Legitimate - Precision: {precision[0]:.3f}, Recall: {recall[0]:.3f}\")\n",
        "            print(f\"   Phishing   - Precision: {precision[1]:.3f}, Recall: {recall[1]:.3f}\")\n",
        "            print(f\"   FPR: {fpr:.3f}, FNR: {fnr:.3f}\")\n",
        "\n",
        "            # Check for XGBoost specific issue\n",
        "            if model_key == 'baseline_xgb' and accuracy < 0.6:\n",
        "                print(f\"   âš ï¸ WARNING: XGBoost accuracy is suspiciously low ({accuracy:.3f})\")\n",
        "                print(f\"      This might indicate a configuration issue.\")\n",
        "                print(f\"      Check: learning rate, n_estimators, or data preprocessing\")\n",
        "\n",
        "        return results, vectorizer\n",
        "\n",
        "    def train_lstm_model(self, X_train_text, X_val_text, y_train, y_val,\n",
        "                        X_train_add, X_val_add, use_additional_features):\n",
        "        \"\"\"Train BiLSTM model\"\"\"\n",
        "        print(\"\\nðŸ”¤ TRAINING BiLSTM MODEL\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Create vocabulary\n",
        "        from collections import Counter\n",
        "\n",
        "        # Simple tokenization\n",
        "        def simple_tokenize(text):\n",
        "            return text.lower().split()\n",
        "\n",
        "        # Build vocabulary\n",
        "        word_counts = Counter()\n",
        "        for text in X_train_text:\n",
        "            word_counts.update(simple_tokenize(str(text)))\n",
        "\n",
        "        # Create vocab mapping\n",
        "        vocab = ['<PAD>', '<UNK>'] + [word for word, _ in word_counts.most_common(10000)]\n",
        "        word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "        # Convert texts to indices\n",
        "        def text_to_indices(text, max_length=512):\n",
        "            tokens = simple_tokenize(str(text))[:max_length]\n",
        "            indices = [word_to_idx.get(token, 1) for token in tokens]\n",
        "            return indices\n",
        "\n",
        "        # Process training data\n",
        "        X_train_indices = [text_to_indices(text) for text in X_train_text]\n",
        "        X_val_indices = [text_to_indices(text) for text in X_val_text]\n",
        "\n",
        "        # Pad sequences\n",
        "        def pad_sequences(sequences, max_length=MAX_LENGTH):\n",
        "            padded = []\n",
        "            for seq in sequences:\n",
        "                if len(seq) < max_length:\n",
        "                    seq.extend([0] * (max_length - len(seq)))\n",
        "                else:\n",
        "                    seq = seq[:max_length]\n",
        "                padded.append(seq)\n",
        "            return np.array(padded)\n",
        "\n",
        "        X_train_padded = pad_sequences(X_train_indices)\n",
        "        X_val_padded = pad_sequences(X_val_indices)\n",
        "\n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.tensor(X_train_padded, dtype=torch.long)\n",
        "        X_val_tensor = torch.tensor(X_val_padded, dtype=torch.long)\n",
        "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "        additional_features_dim = 0\n",
        "        if use_additional_features and X_train_add is not None:\n",
        "            additional_features_dim = X_train_add.shape[1]\n",
        "            X_train_add_tensor = torch.tensor(X_train_add, dtype=torch.float)\n",
        "            X_val_add_tensor = torch.tensor(X_val_add, dtype=torch.float)\n",
        "        else:\n",
        "            X_train_add_tensor = None\n",
        "            X_val_add_tensor = None\n",
        "\n",
        "        # Create model\n",
        "        model = BiLSTMClassifier(\n",
        "            vocab_size=len(vocab),\n",
        "            additional_features_dim=additional_features_dim\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Training setup\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        # Create DataLoader\n",
        "        if X_train_add_tensor is not None:\n",
        "            train_dataset = torch.utils.data.TensorDataset(\n",
        "                X_train_tensor, y_train_tensor, X_train_add_tensor\n",
        "            )\n",
        "        else:\n",
        "            train_dataset = torch.utils.data.TensorDataset(\n",
        "                X_train_tensor, y_train_tensor,\n",
        "                torch.zeros(len(X_train_tensor), 1)  # Dummy tensor\n",
        "            )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "        # Training\n",
        "        print(f\"   Training on {DEVICE}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(EPOCHS):\n",
        "            total_loss = 0\n",
        "            for batch_x, batch_y, batch_add in train_loader:\n",
        "                batch_x = batch_x.to(DEVICE)\n",
        "                batch_y = batch_y.to(DEVICE)\n",
        "\n",
        "                if use_additional_features and X_train_add_tensor is not None:\n",
        "                    batch_add = batch_add.to(DEVICE)\n",
        "                else:\n",
        "                    batch_add = None\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(\n",
        "                    batch_x,\n",
        "                    additional_features=batch_add if use_additional_features else None\n",
        "                )\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"   Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_val_tensor = X_val_tensor.to(DEVICE)\n",
        "            y_val_tensor = y_val_tensor.to(DEVICE)\n",
        "\n",
        "            if use_additional_features and X_val_add_tensor is not None:\n",
        "                X_val_add_tensor = X_val_add_tensor.to(DEVICE)\n",
        "\n",
        "            outputs = model(\n",
        "                X_val_tensor,\n",
        "                additional_features=X_val_add_tensor if use_additional_features else None\n",
        "            )\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            y_pred = predicted.cpu().numpy()\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            y_val, y_pred, average=None, labels=[0, 1]\n",
        "        )\n",
        "        precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(\n",
        "            y_val, y_pred, average='binary'\n",
        "        )\n",
        "        cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
        "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
        "\n",
        "        results = {\n",
        "            'lstm': {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision_avg,\n",
        "                'recall': recall_avg,\n",
        "                'f1_score': f1_avg,\n",
        "                'confusion_matrix': cm.tolist(),\n",
        "                # Class-wise metrics\n",
        "                'legitimate_precision': precision[0],\n",
        "                'legitimate_recall': recall[0],\n",
        "                'legitimate_f1': f1[0],\n",
        "                'legitimate_support': int(support[0]),\n",
        "                'phishing_precision': precision[1],\n",
        "                'phishing_recall': recall[1],\n",
        "                'phishing_f1': f1[1],\n",
        "                'phishing_support': int(support[1]),\n",
        "                # Error rates\n",
        "                'false_positive_rate': fpr,\n",
        "                'false_negative_rate': fnr,\n",
        "                'true_negatives': int(tn),\n",
        "                'false_positives': int(fp),\n",
        "                'false_negatives': int(fn),\n",
        "                'true_positives': int(tp),\n",
        "                'train_time': train_time,\n",
        "                'val_size': len(y_val),\n",
        "                'train_size': len(y_train)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "        print(f\"   F1-Score: {f1_avg:.3f}\")\n",
        "        print(f\"   Training time: {train_time:.1f}s\")\n",
        "\n",
        "        # Print class-wise summary\n",
        "        print(f\"   Legitimate - Precision: {precision[0]:.3f}, Recall: {recall[0]:.3f}\")\n",
        "        print(f\"   Phishing   - Precision: {precision[1]:.3f}, Recall: {recall[1]:.3f}\")\n",
        "        print(f\"   FPR: {fpr:.3f}, FNR: {fnr:.3f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def train_transformer_models(self, X_train_text, X_val_text, y_train, y_val,\n",
        "                               X_train_add, X_val_add, use_additional_features):\n",
        "        \"\"\"Train BERT-based models\"\"\"\n",
        "        print(\"\\nðŸ¤— TRAINING TRANSFORMER MODELS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Determine which models to train\n",
        "        transformer_models = ['bert'] + CYBER_MODELS_TO_USE\n",
        "\n",
        "        for model_key in transformer_models:\n",
        "            if model_key not in MODELS_CONFIG:\n",
        "                continue\n",
        "\n",
        "            config = MODELS_CONFIG[model_key]\n",
        "            print(f\"\\nðŸ“Š Training {config['name']}...\")\n",
        "\n",
        "            try:\n",
        "                # Load tokenizer and model\n",
        "                print(f\"   Loading model: {config['model_name']}\")\n",
        "\n",
        "                # Handle different tokenizer types\n",
        "                if model_key in ['securebert']:\n",
        "                    from transformers import RobertaTokenizer\n",
        "                    tokenizer = RobertaTokenizer.from_pretrained(config['model_name'])\n",
        "                else:\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "\n",
        "                additional_features_dim = 0\n",
        "                if use_additional_features and X_train_add is not None:\n",
        "                    additional_features_dim = X_train_add.shape[1]\n",
        "\n",
        "                # Create model\n",
        "                if model_key in ['bert', 'cysecbert', 'secbert']:\n",
        "                    model = EnhancedBERTClassifier(\n",
        "                        config['model_name'],\n",
        "                        additional_features_dim=additional_features_dim\n",
        "                    ).to(DEVICE)\n",
        "                else:\n",
        "                    # For RoBERTa-based models\n",
        "                    from transformers import RobertaForSequenceClassification\n",
        "                    model = RobertaForSequenceClassification.from_pretrained(\n",
        "                        config['model_name'],\n",
        "                        num_labels=2\n",
        "                    ).to(DEVICE)\n",
        "\n",
        "                # Create datasets\n",
        "                train_dataset = PhishingDataset(\n",
        "                    X_train_text, y_train, tokenizer, MAX_LENGTH,\n",
        "                    additional_features=X_train_add\n",
        "                )\n",
        "                val_dataset = PhishingDataset(\n",
        "                    X_val_text, y_val, tokenizer, MAX_LENGTH,\n",
        "                    additional_features=X_val_add\n",
        "                )\n",
        "\n",
        "                # Training setup\n",
        "                optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "                print(f\"   Training on {DEVICE}...\")\n",
        "                start_time = time.time()\n",
        "\n",
        "                model.train()\n",
        "                for epoch in range(EPOCHS):\n",
        "                    total_loss = 0\n",
        "                    for batch in train_loader:\n",
        "                        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        if hasattr(model, 'forward') and model_key in ['bert', 'cysecbert', 'secbert']:\n",
        "                            outputs = model(\n",
        "                                input_ids=batch['input_ids'],\n",
        "                                attention_mask=batch['attention_mask'],\n",
        "                                additional_features=batch.get('additional_features')\n",
        "                            )\n",
        "                        else:\n",
        "                            outputs = model(\n",
        "                                input_ids=batch['input_ids'],\n",
        "                                attention_mask=batch['attention_mask']\n",
        "                            ).logits\n",
        "\n",
        "                        loss = nn.CrossEntropyLoss()(outputs, batch['labels'])\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                    print(f\"   Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "                train_time = time.time() - start_time\n",
        "\n",
        "                # Evaluation on validation set\n",
        "                model.eval()\n",
        "                val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "                all_predictions = []\n",
        "                all_labels = []\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_loader:\n",
        "                        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "                        if hasattr(model, 'forward') and model_key in ['bert', 'cysecbert', 'secbert']:\n",
        "                            outputs = model(\n",
        "                                input_ids=batch['input_ids'],\n",
        "                                attention_mask=batch['attention_mask'],\n",
        "                                additional_features=batch.get('additional_features')\n",
        "                            )\n",
        "                        else:\n",
        "                            outputs = model(\n",
        "                                input_ids=batch['input_ids'],\n",
        "                                attention_mask=batch['attention_mask']\n",
        "                            ).logits\n",
        "\n",
        "                        predictions = torch.argmax(outputs, dim=-1)\n",
        "                        all_predictions.extend(predictions.cpu().numpy())\n",
        "                        all_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(all_labels, all_predictions)\n",
        "                precision, recall, f1, support = precision_recall_fscore_support(\n",
        "                    all_labels, all_predictions, average=None, labels=[0, 1]\n",
        "                )\n",
        "                precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(\n",
        "                    all_labels, all_predictions, average='binary'\n",
        "                )\n",
        "                cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "                # Calculate additional metrics\n",
        "                tn, fp, fn, tp = cm.ravel()\n",
        "                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
        "                fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
        "\n",
        "                results[model_key] = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision_avg,\n",
        "                    'recall': recall_avg,\n",
        "                    'f1_score': f1_avg,\n",
        "                    'confusion_matrix': cm.tolist(),\n",
        "                    # Class-wise metrics\n",
        "                    'legitimate_precision': precision[0],\n",
        "                    'legitimate_recall': recall[0],\n",
        "                    'legitimate_f1': f1[0],\n",
        "                    'legitimate_support': int(support[0]),\n",
        "                    'phishing_precision': precision[1],\n",
        "                    'phishing_recall': recall[1],\n",
        "                    'phishing_f1': f1[1],\n",
        "                    'phishing_support': int(support[1]),\n",
        "                    # Error rates\n",
        "                    'false_positive_rate': fpr,\n",
        "                    'false_negative_rate': fnr,\n",
        "                    'true_negatives': int(tn),\n",
        "                    'false_positives': int(fp),\n",
        "                    'false_negatives': int(fn),\n",
        "                    'true_positives': int(tp),\n",
        "                    'train_time': train_time,\n",
        "                    'val_size': len(all_labels),\n",
        "                    'train_size': len(y_train)\n",
        "                }\n",
        "\n",
        "                print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "                print(f\"   F1-Score: {f1_avg:.3f}\")\n",
        "                print(f\"   Training time: {train_time:.1f}s\")\n",
        "\n",
        "                # Print class-wise summary\n",
        "                print(f\"   Legitimate - Precision: {precision[0]:.3f}, Recall: {recall[0]:.3f}\")\n",
        "                print(f\"   Phishing   - Precision: {precision[1]:.3f}, Recall: {recall[1]:.3f}\")\n",
        "                print(f\"   FPR: {fpr:.3f}, FNR: {fnr:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   âš ï¸ Error training {config['name']}: {str(e)}\")\n",
        "                print(f\"   Skipping this model...\")\n",
        "                continue\n",
        "\n",
        "            finally:\n",
        "                # Clear GPU memory\n",
        "                if 'model' in locals():\n",
        "                    del model\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_all_models(self):\n",
        "        \"\"\"Main evaluation function with matched sampling\"\"\"\n",
        "        print(\"\\nðŸš€ MULTI-MODEL EVALUATION WITH MATCHED SAMPLING\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Test configurations\n",
        "        configurations = [\n",
        "            {'name': 'Without Additional Features', 'use_features': False},\n",
        "            {'name': 'With Additional Features', 'use_features': True}\n",
        "        ]\n",
        "\n",
        "        all_results = {}\n",
        "\n",
        "        for config in configurations:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"ðŸ“Š CONFIGURATION: {config['name']}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Prepare features\n",
        "            X_text, X_additional, y = self.prepare_features(config['use_features'])\n",
        "\n",
        "            # Create train/validation split matching fine-tuning\n",
        "            X_train_text, X_val_text, y_train, y_val, X_train_add, X_val_add = \\\n",
        "                self.create_train_val_split(X_text, X_additional, y)\n",
        "\n",
        "            # Train baseline models\n",
        "            baseline_results, _ = self.train_baseline_models(\n",
        "                X_train_text, X_val_text, y_train, y_val,\n",
        "                X_train_add, X_val_add, config['use_features']\n",
        "            )\n",
        "\n",
        "            # Train LSTM\n",
        "            lstm_results = self.train_lstm_model(\n",
        "                X_train_text, X_val_text, y_train, y_val,\n",
        "                X_train_add, X_val_add, config['use_features']\n",
        "            )\n",
        "\n",
        "            # Train transformers\n",
        "            transformer_results = self.train_transformer_models(\n",
        "                X_train_text, X_val_text, y_train, y_val,\n",
        "                X_train_add, X_val_add, config['use_features']\n",
        "            )\n",
        "\n",
        "            # Combine results\n",
        "            config_results = {\n",
        "                **baseline_results,\n",
        "                **lstm_results,\n",
        "                **transformer_results\n",
        "            }\n",
        "\n",
        "            all_results[config['name']] = config_results\n",
        "\n",
        "        self.results = all_results\n",
        "        return all_results\n",
        "\n",
        "    def create_class_wise_f1_comparison(self):\n",
        "        \"\"\"Create a focused comparison of class-wise F1 scores\"\"\"\n",
        "        print(\"\\nðŸ“Š CLASS-WISE F1 SCORE COMPARISON\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Critical for understanding model performance on each class:\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Organize data by configuration\n",
        "        for config_name in self.results.keys():\n",
        "            print(f\"\\n{config_name}:\")\n",
        "            print(f\"{'Model':<30} {'Legit F1':>10} {'Phish F1':>10} {'Avg F1':>10} {'Difference':>12}\")\n",
        "            print(\"-\" * 75)\n",
        "\n",
        "            models_data = []\n",
        "            for model_name, metrics in self.results[config_name].items():\n",
        "                if model_name in MODELS_CONFIG:\n",
        "                    legit_f1 = metrics['legitimate_f1']\n",
        "                    phish_f1 = metrics['phishing_f1']\n",
        "                    avg_f1 = metrics['f1_score']\n",
        "                    diff = abs(legit_f1 - phish_f1)\n",
        "\n",
        "                    models_data.append({\n",
        "                        'name': MODELS_CONFIG[model_name]['name'],\n",
        "                        'legit_f1': legit_f1,\n",
        "                        'phish_f1': phish_f1,\n",
        "                        'avg_f1': avg_f1,\n",
        "                        'diff': diff\n",
        "                    })\n",
        "\n",
        "            # Sort by average F1 score\n",
        "            models_data.sort(key=lambda x: x['avg_f1'], reverse=True)\n",
        "\n",
        "            for model in models_data:\n",
        "                balance_indicator = \"âœ“\" if model['diff'] < 0.05 else \"âš \" if model['diff'] < 0.1 else \"âœ—\"\n",
        "                print(f\"{model['name']:<30} {model['legit_f1']:>10.3f} {model['phish_f1']:>10.3f} \"\n",
        "                      f\"{model['avg_f1']:>10.3f} {model['diff']:>10.3f} {balance_indicator}\")\n",
        "\n",
        "        print(\"\\n Legend: âœ“ Well balanced (<0.05 diff), âš  Some imbalance (0.05-0.1), âœ— High imbalance (>0.1)\")\n",
        "\n",
        "    def generate_comparison_report(self):\n",
        "        \"\"\"Generate comprehensive comparison report with class-wise metrics\"\"\"\n",
        "        print(\"\\nðŸ“Š COMPREHENSIVE COMPARISON REPORT WITH CLASS-WISE METRICS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Create comparison tables for each configuration\n",
        "        for config_name, models in self.results.items():\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"ðŸ“Š {config_name}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Detailed metrics table\n",
        "            detailed_data = []\n",
        "\n",
        "            for model_name, metrics in models.items():\n",
        "                if model_name in MODELS_CONFIG:\n",
        "                    detailed_data.append({\n",
        "                        'Model': MODELS_CONFIG[model_name]['name'],\n",
        "                        'Overall Acc': f\"{metrics['accuracy']:.3f}\",\n",
        "                        'Legit Prec': f\"{metrics['legitimate_precision']:.3f}\",\n",
        "                        'Legit Rec': f\"{metrics['legitimate_recall']:.3f}\",\n",
        "                        'Legit F1': f\"{metrics['legitimate_f1']:.3f}\",\n",
        "                        'Phish Prec': f\"{metrics['phishing_precision']:.3f}\",\n",
        "                        'Phish Rec': f\"{metrics['phishing_recall']:.3f}\",\n",
        "                        'Phish F1': f\"{metrics['phishing_f1']:.3f}\",\n",
        "                        'FPR': f\"{metrics['false_positive_rate']:.3f}\",\n",
        "                        'FNR': f\"{metrics['false_negative_rate']:.3f}\"\n",
        "                    })\n",
        "\n",
        "            df_detailed = pd.DataFrame(detailed_data)\n",
        "            print(\"\\n\" + df_detailed.to_string(index=False))\n",
        "\n",
        "            # Confusion Matrix Details (shortened version)\n",
        "            print(f\"\\nðŸ“Š CONFUSION MATRICES - {config_name}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            for model_name, metrics in models.items():\n",
        "                if model_name in MODELS_CONFIG:\n",
        "                    print(f\"\\n{MODELS_CONFIG[model_name]['name']}:\")\n",
        "                    print(f\"   TN: {metrics['true_negatives']:4d}  FP: {metrics['false_positives']:4d}  |  FPR: {metrics['false_positive_rate']:.3f}\")\n",
        "                    print(f\"   FN: {metrics['false_negatives']:4d}  TP: {metrics['true_positives']:4d}  |  FNR: {metrics['false_negative_rate']:.3f}\")\n",
        "\n",
        "        # Add class-wise F1 comparison\n",
        "        self.create_class_wise_f1_comparison()\n",
        "\n",
        "        # Critical Metrics Summary (shortened)\n",
        "        print(\"\\nðŸš¨ CRITICAL METRICS SUMMARY\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        for config_name in self.results.keys():\n",
        "            print(f\"\\n{config_name}:\")\n",
        "            models = self.results[config_name]\n",
        "\n",
        "            # Best by combined error rate\n",
        "            best_balanced = min(models.items(),\n",
        "                              key=lambda x: x[1]['false_positive_rate'] + x[1]['false_negative_rate'])\n",
        "            combined_error = best_balanced[1]['false_positive_rate'] + best_balanced[1]['false_negative_rate']\n",
        "            print(f\"   Best Overall: {MODELS_CONFIG[best_balanced[0]]['name']} (FPR+FNR: {combined_error:.3f})\")\n",
        "\n",
        "        # Save results\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Save full results JSON\n",
        "        results_file = f\"detailed_evaluation_results_{timestamp}.json\"\n",
        "        with open(results_file, 'w') as f:\n",
        "            json.dump(self.results, f, indent=2)\n",
        "        print(f\"\\nðŸ’¾ Detailed results saved to: {results_file}\")\n",
        "\n",
        "        # Save summary CSV\n",
        "        summary_data = []\n",
        "        for config_name, models in self.results.items():\n",
        "            for model_name, metrics in models.items():\n",
        "                if model_name in MODELS_CONFIG:\n",
        "                    summary_data.append({\n",
        "                        'Configuration': config_name,\n",
        "                        'Model': MODELS_CONFIG[model_name]['name'],\n",
        "                        'Accuracy': f\"{metrics['accuracy']:.3f}\",\n",
        "                        'Legit_F1': f\"{metrics['legitimate_f1']:.3f}\",\n",
        "                        'Phish_F1': f\"{metrics['phishing_f1']:.3f}\",\n",
        "                        'Avg_F1': f\"{metrics['f1_score']:.3f}\",\n",
        "                        'FPR': f\"{metrics['false_positive_rate']:.3f}\",\n",
        "                        'FNR': f\"{metrics['false_negative_rate']:.3f}\",\n",
        "                        'Train_Time_sec': f\"{metrics['train_time']:.1f}\"\n",
        "                    })\n",
        "\n",
        "        df_summary = pd.DataFrame(summary_data)\n",
        "        csv_file = f\"class_wise_evaluation_{timestamp}.csv\"\n",
        "        df_summary.to_csv(csv_file, index=False)\n",
        "        print(f\"ðŸ“Š Class-wise metrics saved to: {csv_file}\")\n",
        "\n",
        "        # Generate final results table\n",
        "        self.generate_final_results_table()\n",
        "\n",
        "        return df_summary\n",
        "\n",
        "# ==========================================\n",
        "# MAIN EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "def analyze_suspicious_results(results):\n",
        "    \"\"\"Analyze results for suspicious patterns\"\"\"\n",
        "    print(\"\\nâš ï¸  RESULTS VALIDATION\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    warnings = []\n",
        "\n",
        "    for config_name, models in results.items():\n",
        "        for model_name, metrics in models.items():\n",
        "            model_display_name = MODELS_CONFIG.get(model_name, {}).get('name', model_name)\n",
        "\n",
        "            # Check for models predicting only one class\n",
        "            if metrics['phishing_recall'] == 0:\n",
        "                warnings.append(f\"{config_name} - {model_display_name}: Not detecting ANY phishing emails (100% FNR)!\")\n",
        "            elif metrics['legitimate_recall'] == 0:\n",
        "                warnings.append(f\"{config_name} - {model_display_name}: Not detecting ANY legitimate emails (100% FPR)!\")\n",
        "\n",
        "            # Check for extreme imbalances\n",
        "            if metrics['false_positive_rate'] == 0 and metrics['false_negative_rate'] > 0.5:\n",
        "                warnings.append(f\"{config_name} - {model_display_name}: Zero FPR but very high FNR ({metrics['false_negative_rate']:.1%})\")\n",
        "            elif metrics['false_negative_rate'] == 0 and metrics['false_positive_rate'] > 0.5:\n",
        "                warnings.append(f\"{config_name} - {model_display_name}: Zero FNR but very high FPR ({metrics['false_positive_rate']:.1%})\")\n",
        "\n",
        "            # Check for broken models (accuracy near 50% for binary classification)\n",
        "            if 0.45 < metrics['accuracy'] < 0.55:\n",
        "                warnings.append(f\"{config_name} - {model_display_name}: Near random performance (accuracy: {metrics['accuracy']:.1%})\")\n",
        "\n",
        "    if warnings:\n",
        "        print(\"ðŸš¨ Issues detected:\")\n",
        "        for warning in warnings:\n",
        "            print(f\"   - {warning}\")\n",
        "\n",
        "        # Specific advice for common issues\n",
        "        if any(\"Not detecting ANY phishing emails\" in w for w in warnings):\n",
        "            print(\"\\nðŸ’¡ XGBoost Fix: Try adjusting scale_pos_weight or using sample weights\")\n",
        "        if any(\"Near random performance\" in w for w in warnings):\n",
        "            print(\"\\nðŸ’¡ BiLSTM Fix: May need more epochs, different architecture, or better hyperparameters\")\n",
        "    else:\n",
        "        print(\"âœ… No critical issues detected\")\n",
        "        print(\"   Note: High accuracy (>99%) can be legitimate with good features\")\n",
        "\n",
        "    return warnings\n",
        "\n",
        "def run_matched_evaluation():\n",
        "    \"\"\"Run evaluation with sampling matching fine-tuning\"\"\"\n",
        "\n",
        "    evaluator = MatchedSamplingEvaluator(DATASET_PATH)\n",
        "\n",
        "    # Load data with matched sampling\n",
        "    if not evaluator.load_and_prepare_data():\n",
        "        print(\"âŒ Failed to load dataset\")\n",
        "        return\n",
        "\n",
        "    # Run evaluation\n",
        "    evaluator.evaluate_all_models()\n",
        "\n",
        "    # Generate report\n",
        "    evaluator.generate_comparison_report()\n",
        "\n",
        "    # Analyze for suspicious results\n",
        "    analyze_suspicious_results(evaluator.results)\n",
        "\n",
        "    print(\"\\nâœ… EVALUATION COMPLETED!\")\n",
        "    print(f\"   Matched fine-tuning sampling: {SAMPLES_PER_CLASS} per class\")\n",
        "    print(f\"   Train/Val split preserved: ~4,730/1,000\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Clear GPU cache before starting\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"ðŸš€ Starting Multi-Model Evaluation with Matched Sampling\")\n",
        "    print(f\"   Matching fine-tuning setup: {SAMPLES_PER_CLASS} samples per class\")\n",
        "    print(f\"   Device: {DEVICE}\")\n",
        "\n",
        "    run_matched_evaluation()"
      ]
    }
  ]
}